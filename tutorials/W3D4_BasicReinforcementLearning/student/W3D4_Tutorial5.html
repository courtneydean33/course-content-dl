
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>(Bonus) Tutorial 5: Function approximation — Neuromatch Academy: Deep Learning</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-dl-logo-square-4xp.png" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W3D4_BonusLecture.html" rel="next" title="Bonus Lecture: Chealsea Finn"/>
<link href="W3D4_Tutorial4.html" rel="prev" title="Tutorial 4: Model-Based Reinforcement Learning"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.png"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</input></li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../TechnicalHelp/Links_Policy.html">
   Quick links and policies
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../prereqs/DeepLearning.html">
   Prerequisites and preparatory materials for NMA Deep Learning
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Basics Module
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_BonusLecture.html">
     Bonus Lecture: Yoshua Bengio
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Fine Tuning
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Optimization/chapter_title.html">
   Optimization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Optimization/student/W1D5_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_Regularization/chapter_title.html">
   Regularization (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_Regularization/student/W2D1_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_Regularization/student/W2D1_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/FineTuning.html">
   Deep Learning: The Basics and Fine Tuning Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Convolutional Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ConvnetsAndDlThinking/chapter_title.html">
   Convnets And Dl Thinking (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ConvnetsAndDlThinking/student/W2D2_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ConvnetsAndDlThinking/student/W2D2_Tutorial2.html">
     Tutorial 2: Deep Learning Thinking 1: Cost Functions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ConvnetsAndDlThinking/student/W2D2_BonusLecture.html">
     Bonus Lecture: Kyunghyun Cho
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernConvnets/student/W2D3_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernConvnets/student/W2D3_Tutorial2.html">
     (Bonus) Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_GenerativeModels/chapter_title.html">
   Generative Models (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_GenerativeModels/student/W2D4_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_GenerativeModels/student/W2D4_Tutorial2.html">
     Tutorial 2: Introduction to GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_GenerativeModels/student/W2D4_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_GenerativeModels/student/W2D4_Tutorial4.html">
     (Bonus) Tutorial 4: Deploying Neural Networks on the Web
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_GenerativeModels/student/W2D4_BonusLecture.html">
     Bonus Lecture: Geoffrey Hinton
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Natural Language Processing
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_TimeSeriesAndNaturalLanguageProcessing/chapter_title.html">
   Time Series And Natural Language Processing (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_TimeSeriesAndNaturalLanguageProcessing/student/W2D5_Tutorial1.html">
     Tutorial 1: Introduction to processing time series
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_TimeSeriesAndNaturalLanguageProcessing/student/W2D5_Tutorial2.html">
     Tutorial 2: Time series for Language
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_TimeSeriesAndNaturalLanguageProcessing/student/W2D5_Tutorial3.html">
     (Bonus) Tutorial 3: Multilingual Embeddings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_AttentionAndTransformers/student/W3D1_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_DlThinking2/chapter_title.html">
   Dl Thinking2 (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_DlThinking2/student/W3D2_Tutorial1.html">
     Tutorial 1: Deep Learning Thinking 2: Architectures and Multimodal DL thinking
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/NaturalLanguageProcessing.html">
   Deep Learning: Convnets and NLP
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_UnsupervisedAndSelfSupervisedLearning/student/W3D3_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_UnsupervisedAndSelfSupervisedLearning/student/W3D3_BonusLecture.html">
     Bonus Lecture: Emily Denton &amp; Melanie Mitchell
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Basic Reinforcement Learning (W3D4)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Tutorial4.html">
     Tutorial 4: Model-Based Reinforcement Learning
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     (Bonus) Tutorial 5: Function approximation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_BonusLecture.html">
     Bonus Lecture: Chealsea Finn
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D5_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_ReinforcementLearningForGames/student/W3D5_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_ReinforcementLearningForGames/student/W3D5_BonusLecture.html">
     Bonus Lecture: Amita Kapoor
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/ReinforcementLearning.html">
   Deep Learning: Reinforcement Learning Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/projects_overview.html">
   Project Templates
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ComputerVision/README.html">
     Computer Vision
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/screws.html">
       Something Screwy - image recognition, detection, and classification of screws
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/human_rl.html">
       Using RL to Model Cognitive Tasks
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/sentiment_analysis.html">
       Twitter Sentiment Analysis
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/machine_translation.html">
       Machine Translation
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/Neuroscience/README.html">
     Neuroscience
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/datasets_and_models.html">
   Models and Data sets
  </a>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/course-content-dl" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W3D4_BasicReinforcementLearning/student/W3D4_Tutorial5.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W3D4_BasicReinforcementLearning/student/W3D4_Tutorial5.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   (Bonus) Tutorial 5: Function approximation
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-requirements">
     Install requirements
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#acme-a-research-framework-for-reinforcement-learning">
     Acme: a research framework for reinforcement learning
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#implement-gridworld">
       Implement GridWorld
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#double-click-to-inspect-the-contents-of-this-cell">
<em>
          Double-click
         </em>
         to inspect the contents of this cell.
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualise-gridworlds">
       Visualise GridWorlds
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#look-at-environment-spec">
       Look at
       <code class="docutils literal notranslate">
<span class="pre">
         environment_spec
        </span>
</code>
</a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#note-setup-environment-is-implemented-in-the-same-cell-as-gridworld">
<strong>
          Note:
         </strong>
<code class="docutils literal notranslate">
<span class="pre">
           setup_environment
          </span>
</code>
         is implemented in the same cell as GridWorld.
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#pick-an-action-and-see-the-state-changing">
       Pick an action and see the state changing
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-loop">
       Run loop
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#implement-the-evaluation-loop">
       Implement the evaluation loop
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-function-approximation">
   Section 1: Function Approximation
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-function-approximation">
     Video 1: Function approximation
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-replay-buffers">
     Section 1.1 Replay Buffers
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-nfq-agent">
     Section 1.2: NFQ Agent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-1-implement-nfq">
       Coding Exercise 1.1: Implement NFQ
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-and-evaluate-the-nfq-agent">
       Train and Evaluate the NFQ Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-the-nfq-agent">
         Training the NFQ Agent
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluating-the-agent-set-epsilon-0">
         Evaluating the agent (set
         <span class="math notranslate nohighlight">
          \(\epsilon=0\)
         </span>
         )
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#temporarily-change-epsilon-to-be-more-greedy-remember-to-change-it-back">
           Temporarily change epsilon to be more greedy; remember to change it back.
          </a>
</li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualise-the-learned-q-values">
         Visualise the learned
         <span class="math notranslate nohighlight">
          \(Q\)
         </span>
         values
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluate-the-policy-for-every-state-similar-to-tabular-agents-above">
           Evaluate the policy for every state, similar to tabular agents above.
          </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#compare-the-greedy-and-behaviour-epsilon-greedy-policies">
       Compare the greedy and behaviour (
       <span class="math notranslate nohighlight">
        \(\epsilon\)
       </span>
       -greedy) policies
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#compare-the-greedy-policy-with-the-agent-s-policy">
         Compare the greedy policy with the agent’s policy
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#notice-that-the-agent-s-behavior-policy-has-a-lot-more-randomness-due-to-the-high-epsilon-however-the-greedy-policy-that-s-learned-is-optimal">
           Notice that the agent’s behavior policy has a lot more randomness, due to the high
           <span class="math notranslate nohighlight">
            \(\epsilon\)
           </span>
           . However, the greedy policy that’s learned is optimal.
          </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-deep-q-networks-dqn">
   Section 2: Deep Q-Networks (DQN)
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-deep-q-networks-dqn">
     Video 2: Deep Q-Networks (DQN)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-run-a-dqn-agent">
       Coding Exercise 2.1: Run a DQN Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-and-evaluate-the-dqn-agent">
         Train and evaluate the DQN agent
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
         Visualise the learned
         <span class="math notranslate nohighlight">
          \(Q\)
         </span>
         values
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
           Evaluate the policy for every state, similar to tabular agents above.
          </a>
</li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id3">
         Compare the greedy policy with the agent’s policy
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-learning-the-policy-directly">
   Section 3: Learning the policy directly
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-other-deep-rl-methods">
     Video 3: Other Deep RL Methods
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#cartpole-task">
     Cartpole task
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-a-cartpole-environment-gym-make-cartpole-v1">
       Make a CartPole environment,
       <code class="docutils literal notranslate">
<span class="pre">
         gym.make('CartPole-v1')
        </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-policy-gradient">
     Section 3.1: Policy gradient
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-the-hyperparameters-for-policy-gradient">
       Set the hyperparameters for Policy Gradient
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-1-creating-a-simple-neural-network">
       Coding Exercise 3.1: Creating a simple neural network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#select-action">
       Select Action
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#update-policy">
       Update policy
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#reward-g-t">
         Reward
         <span class="math notranslate nohighlight">
          \(G_t\)
         </span>
</a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#update-policy-equation">
         Update Policy: equation
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-the-model">
       Run the model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-results">
       Plot the results
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-training-performance-for-policy-gradient">
         Plot the training performance for policy gradient
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-1-explore-different-hyperparameters">
       Exercise 3.1: Explore different hyperparameters.
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-actor-critic">
     Section 3.2: Actor-critic
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-the-hyperparameters-for-actor-critic">
       Set the hyperparameters for Actor Critic
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#actor-critic-network">
       Actor Critic Network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id4">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id5">
       Run the model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id6">
       Plot the results
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-training-performance-for-actor-critic">
         Plot the training performance for Actor Critic
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-2-1-effect-of-episodes-on-performance">
       Exercise 3.2.1: Effect of episodes on performance
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-2-2-effect-of-learning-rate-on-performance">
       Exercise 3.2.2: Effect of learning rate on performance
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-rl-in-the-real-world">
   Section 4: RL in the real world
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-real-world-applications-and-ethics">
     Video 4: Real-world applications and ethics
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-4-group-discussion">
     Exercise 4: Group discussion
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-how-to-learn-more">
   Section 5: How to learn more
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-how-to-learn-more">
     Video 5: How to learn more
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#appendix-and-further-reading">
   Appendix and further reading
  </a>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>(Bonus) Tutorial 5: Function approximation</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   (Bonus) Tutorial 5: Function approximation
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-requirements">
     Install requirements
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#acme-a-research-framework-for-reinforcement-learning">
     Acme: a research framework for reinforcement learning
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#implement-gridworld">
       Implement GridWorld
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#double-click-to-inspect-the-contents-of-this-cell">
<em>
          Double-click
         </em>
         to inspect the contents of this cell.
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualise-gridworlds">
       Visualise GridWorlds
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#look-at-environment-spec">
       Look at
       <code class="docutils literal notranslate">
<span class="pre">
         environment_spec
        </span>
</code>
</a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#note-setup-environment-is-implemented-in-the-same-cell-as-gridworld">
<strong>
          Note:
         </strong>
<code class="docutils literal notranslate">
<span class="pre">
           setup_environment
          </span>
</code>
         is implemented in the same cell as GridWorld.
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#pick-an-action-and-see-the-state-changing">
       Pick an action and see the state changing
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-loop">
       Run loop
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#implement-the-evaluation-loop">
       Implement the evaluation loop
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-function-approximation">
   Section 1: Function Approximation
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-function-approximation">
     Video 1: Function approximation
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-replay-buffers">
     Section 1.1 Replay Buffers
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-nfq-agent">
     Section 1.2: NFQ Agent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-1-implement-nfq">
       Coding Exercise 1.1: Implement NFQ
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-and-evaluate-the-nfq-agent">
       Train and Evaluate the NFQ Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-the-nfq-agent">
         Training the NFQ Agent
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluating-the-agent-set-epsilon-0">
         Evaluating the agent (set
         <span class="math notranslate nohighlight">
          \(\epsilon=0\)
         </span>
         )
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#temporarily-change-epsilon-to-be-more-greedy-remember-to-change-it-back">
           Temporarily change epsilon to be more greedy; remember to change it back.
          </a>
</li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualise-the-learned-q-values">
         Visualise the learned
         <span class="math notranslate nohighlight">
          \(Q\)
         </span>
         values
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluate-the-policy-for-every-state-similar-to-tabular-agents-above">
           Evaluate the policy for every state, similar to tabular agents above.
          </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#compare-the-greedy-and-behaviour-epsilon-greedy-policies">
       Compare the greedy and behaviour (
       <span class="math notranslate nohighlight">
        \(\epsilon\)
       </span>
       -greedy) policies
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#compare-the-greedy-policy-with-the-agent-s-policy">
         Compare the greedy policy with the agent’s policy
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#notice-that-the-agent-s-behavior-policy-has-a-lot-more-randomness-due-to-the-high-epsilon-however-the-greedy-policy-that-s-learned-is-optimal">
           Notice that the agent’s behavior policy has a lot more randomness, due to the high
           <span class="math notranslate nohighlight">
            \(\epsilon\)
           </span>
           . However, the greedy policy that’s learned is optimal.
          </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-deep-q-networks-dqn">
   Section 2: Deep Q-Networks (DQN)
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-deep-q-networks-dqn">
     Video 2: Deep Q-Networks (DQN)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-run-a-dqn-agent">
       Coding Exercise 2.1: Run a DQN Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-and-evaluate-the-dqn-agent">
         Train and evaluate the DQN agent
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
         Visualise the learned
         <span class="math notranslate nohighlight">
          \(Q\)
         </span>
         values
        </a>
<ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
           Evaluate the policy for every state, similar to tabular agents above.
          </a>
</li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id3">
         Compare the greedy policy with the agent’s policy
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-learning-the-policy-directly">
   Section 3: Learning the policy directly
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-other-deep-rl-methods">
     Video 3: Other Deep RL Methods
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#cartpole-task">
     Cartpole task
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-a-cartpole-environment-gym-make-cartpole-v1">
       Make a CartPole environment,
       <code class="docutils literal notranslate">
<span class="pre">
         gym.make('CartPole-v1')
        </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-policy-gradient">
     Section 3.1: Policy gradient
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-the-hyperparameters-for-policy-gradient">
       Set the hyperparameters for Policy Gradient
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-1-creating-a-simple-neural-network">
       Coding Exercise 3.1: Creating a simple neural network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#select-action">
       Select Action
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#update-policy">
       Update policy
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#reward-g-t">
         Reward
         <span class="math notranslate nohighlight">
          \(G_t\)
         </span>
</a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#update-policy-equation">
         Update Policy: equation
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-the-model">
       Run the model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-results">
       Plot the results
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-training-performance-for-policy-gradient">
         Plot the training performance for policy gradient
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-1-explore-different-hyperparameters">
       Exercise 3.1: Explore different hyperparameters.
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-actor-critic">
     Section 3.2: Actor-critic
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-the-hyperparameters-for-actor-critic">
       Set the hyperparameters for Actor Critic
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#actor-critic-network">
       Actor Critic Network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id4">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id5">
       Run the model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id6">
       Plot the results
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-training-performance-for-actor-critic">
         Plot the training performance for Actor Critic
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-2-1-effect-of-episodes-on-performance">
       Exercise 3.2.1: Effect of episodes on performance
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-2-2-effect-of-learning-rate-on-performance">
       Exercise 3.2.2: Effect of learning rate on performance
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-rl-in-the-real-world">
   Section 4: RL in the real world
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-real-world-applications-and-ethics">
     Video 4: Real-world applications and ethics
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-4-group-discussion">
     Exercise 4: Group discussion
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-how-to-learn-more">
   Section 5: How to learn more
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-how-to-learn-more">
     Video 5: How to learn more
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#appendix-and-further-reading">
   Appendix and further reading
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D4_BasicReinforcementLearning/student/W3D4_Tutorial5.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_BasicReinforcementLearning/student/W3D4_Tutorial5.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<div class="section" id="bonus-tutorial-5-function-approximation">
<h1>(Bonus) Tutorial 5: Function approximation<a class="headerlink" href="#bonus-tutorial-5-function-approximation" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 3, Day 4: Basic Reinforcement Learning (RL)</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Marcelo G Mattar, Eric DeWitt, Matt Krause, Matthew Sargent, Anoop Kulkarni, Sowmya Parthiban, Feryal Behbahani, Jane Wang</p>
<p><strong>Content reviewers:</strong> Ella Batty, Byron Galbraith, Michael Waskom, Ezekiel Williams, Mehul Rastogi, Lily Cheng, Roberto Guidotti, Arush Tagade, Kelson Shilling-Scrivo</p>
<p><strong>Production editors:</strong> Gagana B, Spiros Chavlis</p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>This whole notebook contains bonus material to get a better sense of more complex reinforcement learning models as well as real world applications of RL. Previously, we implemented fundemental ideas of RL in basic Python. Here, we will show how these can be implemented using the <a class="reference external" href="https://www.deepmind.com/publications/acme-a-new-framework-for-distributed-reinforcement-learning">Acme</a> library by DeepMind. For the project on GitHub see <a class="reference external" href="https://github.com/deepmind/acme">here</a>.</p>
<p>By the end of the tutorial, you should be able to:</p>
<ol class="simple">
<li><p>Implement, train, and test a NFQ Agent</p></li>
<li><p>Use a Deep Q-network</p></li>
<li><p>Learn the Policy Gradient and the Actor Critic</p></li>
<li><p>Use RL in real-world applications</p></li>
</ol>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/m3kqy/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
<p>These are the slides for all videos in this tutorial. If you want to locally download the slides, click <a class="reference external" href="https://osf.io/m3kqy/download">here</a>.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>Run the following <em>Setup</em> cells in order to set up needed functions. Don’t worry about the code for now!</p>
<p><strong>Note:</strong> There is an issue with some images not showing up if you’re using a Safari browser. Please switch to Chrome if this is the case.</p>
<div class="section" id="install-requirements">
<h2>Install requirements<a class="headerlink" href="#install-requirements" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install requirements</span>

<span class="c1"># @markdown We install the acme library, see [here](https://github.com/deepmind/acme) for more info.</span>

<span class="c1"># @markdown **WARNING:** There may be *errors* and/or *warnings* reported during the installation. However, they should be ignored.</span>

<span class="o">!</span>pip install --upgrade pip --quiet
<span class="o">!</span>pip install imageio --quiet
<span class="o">!</span>pip install imageio-ffmpeg --quiet
<span class="o">!</span>pip install gym --quiet
<span class="o">!</span>pip install enum34 --quiet
<span class="o">!</span>pip install pandas --quiet
<span class="o">!</span>pip install <span class="nv">grpcio</span><span class="o">==</span><span class="m">1</span>.34.0 --quiet
<span class="o">!</span>pip install typing --quiet
<span class="o">!</span>pip install einops --quiet
<span class="o">!</span>pip install dm-acme<span class="o">[</span>reverb<span class="o">]</span> --quiet
<span class="o">!</span>pip install dm-acme<span class="o">[</span>jax,tensorflow<span class="o">]</span> --quiet
<span class="o">!</span>pip install dm-acme<span class="o">[</span>envs<span class="o">]</span> --quiet
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Red">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.</span>
<span class="-Color -Color-Red">konoha 4.6.5 requires importlib-metadata&lt;4.0.0,&gt;=3.7.0, but you have importlib-metadata 4.12.0 which is incompatible.</span>
<span class="-Color -Color-Red">flake8 4.0.1 requires importlib-metadata&lt;4.3; python_version &lt; "3.8", but you have importlib-metadata 4.12.0 which is incompatible.</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: dm-acme 0.4.0 does not provide the extra 'reverb'</span>

</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: dm-acme 0.4.0 does not provide the extra 'tensorflow'</span>

</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import modules</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">enum</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">acme</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">dm_env</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Sequence</span>

<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>

<span class="kn">from</span> <span class="nn">acme</span> <span class="kn">import</span> <span class="n">specs</span>
<span class="kn">from</span> <span class="nn">acme</span> <span class="kn">import</span> <span class="n">wrappers</span>
<span class="kn">from</span> <span class="nn">acme.utils</span> <span class="kn">import</span> <span class="n">tree_utils</span>
<span class="kn">from</span> <span class="nn">acme.utils</span> <span class="kn">import</span> <span class="n">loggers</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>  <span class="c1"># Interactive display</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'image'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'Blues'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper Functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<p>Implement helpers for value visualisation</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper Functions</span>
<span class="c1"># @markdown Implement helpers for value visualisation</span>

<span class="n">map_from_action_to_subplot</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)[</span><span class="n">a</span><span class="p">]</span>
<span class="n">map_from_action_to_name</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="p">(</span><span class="s2">"up"</span><span class="p">,</span> <span class="s2">"right"</span><span class="p">,</span> <span class="s2">"down"</span><span class="p">,</span> <span class="s2">"left"</span><span class="p">)[</span><span class="n">a</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">plot_values</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">'pink'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plots incoming values</span>

<span class="sd">  Args:</span>
<span class="sd">    values: List</span>
<span class="sd">      List of values to be plotted</span>
<span class="sd">    colormap: String</span>
<span class="sd">      Defines colormap of plot</span>
<span class="sd">    vmin: Integer</span>
<span class="sd">      Smallest possible value within "values"</span>
<span class="sd">    vmax: Integer</span>
<span class="sd">      Highest possible value within "values"</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">"nearest"</span><span class="p">,</span>
             <span class="n">cmap</span><span class="o">=</span><span class="n">colormap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">plot_state_value</span><span class="p">(</span><span class="n">state_values</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to plot state value</span>

<span class="sd">  Args:</span>
<span class="sd">    state_values: np.ndarray</span>
<span class="sd">      Action values with shape (9, 10, 4)</span>
<span class="sd">    epsilon: Float</span>
<span class="sd">      Sets the exploitation-exploration control hyperparameter [default=0.1]</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">q</span> <span class="o">=</span> <span class="n">state_values</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
  <span class="n">vmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">state_values</span><span class="p">)</span>
  <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">state_values</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plot_values</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">'summer'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"$v(s)$"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_action_values</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to plot action value</span>

<span class="sd">  Args:</span>
<span class="sd">    action_values: np.ndarray</span>
<span class="sd">      Action values with shape (9, 10, 4)</span>
<span class="sd">    epsilon: Float</span>
<span class="sd">      Sets the exploitation-exploration control hyperparameter [default=0.1]</span>
<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">q</span> <span class="o">=</span> <span class="n">action_values</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
  <span class="n">vmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
  <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
  <span class="n">dif</span> <span class="o">=</span> <span class="n">vmax</span> <span class="o">-</span> <span class="n">vmin</span>
  <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">map_from_action_to_subplot</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

    <span class="n">plot_values</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span> <span class="o">-</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">dif</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">dif</span><span class="p">)</span>
    <span class="n">action_name</span> <span class="o">=</span> <span class="n">map_from_action_to_name</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$q(s, \mathrm{"</span> <span class="o">+</span> <span class="n">action_name</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">"})$"</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plot_values</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">'summer'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"$v(s)$"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h2>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="c1"># @markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># For DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function that controls randomness. NumPy and random modules must be imported.</span>

<span class="sd">  Args:</span>
<span class="sd">    seed : Integer</span>
<span class="sd">      A non-negative integer that defines the random state. Default is `None`.</span>
<span class="sd">    seed_torch : Boolean</span>
<span class="sd">      If `True` sets the random seed for pytorch tensors, so pytorch module</span>
<span class="sd">      must be imported. Default is `True`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>


<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  DataLoader will reseed workers following randomness in</span>
<span class="sd">  multi-process data loading algorithm.</span>

<span class="sd">  Args:</span>
<span class="sd">    worker_id: integer</span>
<span class="sd">      ID of subprocess to seed. 0 means that</span>
<span class="sd">      the data will be loaded in the main process</span>
<span class="sd">      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h2>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU). Execute `set_device()`</span>
<span class="c1"># especially if torch modules used.</span>

<span class="c1"># Inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="sd">"""</span>
<span class="sd">  Set the device. CUDA if available, CPU otherwise</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -&gt; `Change runtime type.`  select `GPU` 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="acme-a-research-framework-for-reinforcement-learning">
<h2>Acme: a research framework for reinforcement learning<a class="headerlink" href="#acme-a-research-framework-for-reinforcement-learning" title="Permalink to this headline">¶</a></h2>
<p><strong>Acme</strong> is a library of reinforcement learning (RL) agents and agent building blocks by Google DeepMind. Acme strives to expose simple, efficient, and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research. The design of Acme also attempts to provide multiple points of entry to the RL problem at differing levels of complexity.</p>
<p>For more information see the github’s repository <a class="reference external" href="https://github.com/deepmind/acme">deepmind/acme</a>.</p>
<p>For this practical session, we will focus on a <strong>simple grid world</strong> environment, which consists of a 9 x 10 grid of wall and empty cells - depicted in black and white, respectively. The smiling agent starts from an initial location and needs to navigate to reach the goal square.</p>
<center><img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_BasicReinforcementLearning/static/tabular_RL_loop.png" width="500"/></center>
<p>Below you will find an implementation of this Gridworld as a <code class="docutils literal notranslate"><span class="pre">dm_env.Environment</span></code>.</p>
<p>There is no coding in this section, but if you want, you can look over the provided code so that you can familiarize yourself with an example of how to set up a <strong>grid world</strong> environment.</p>
<div class="section" id="implement-gridworld">
<h3>Implement GridWorld<a class="headerlink" href="#implement-gridworld" title="Permalink to this headline">¶</a></h3>
<div class="section" id="double-click-to-inspect-the-contents-of-this-cell">
<h4><em>Double-click</em> to inspect the contents of this cell.<a class="headerlink" href="#double-click-to-inspect-the-contents-of-this-cell" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Implement GridWorld</span>

<span class="c1"># @markdown ##### *Double-click* to inspect the contents of this cell.</span>


<span class="k">class</span> <span class="nc">ObservationType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Class to examine observation type including goal position and state index;</span>
<span class="sd">  Attributes:</span>
<span class="sd">    observation_type: Enum</span>
<span class="sd">        Enum observation type to use. One of:</span>
<span class="sd">        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.</span>
<span class="sd">        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the</span>
<span class="sd">          agent is and 0 elsewhere.</span>
<span class="sd">        * ObservationType.GRID: NxNx3 float32 grid of feature channels.</span>
<span class="sd">          First channel contains walls (1 if wall, 0 otherwise), second the</span>
<span class="sd">          agent position (1 if agent, 0 otherwise) and third goal position</span>
<span class="sd">          (1 if goal, 0 otherwise)</span>
<span class="sd">        * ObservationType.AGENT_GOAL_POS: float32 tuple with</span>
<span class="sd">          (agent_y, agent_x, goal_y, goal_x)</span>
<span class="sd">  """</span>
  <span class="n">STATE_INDEX</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
  <span class="n">AGENT_ONEHOT</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
  <span class="n">GRID</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
  <span class="n">AGENT_GOAL_POS</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">dm_env</span><span class="o">.</span><span class="n">Environment</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Build a grid environment.</span>
<span class="sd">  Simple gridworld defined by a map layout, a start and a goal state.</span>
<span class="sd">  Layout should be a NxN grid, containing:</span>
<span class="sd">      * 0: Empty</span>
<span class="sd">      * -1: Wall</span>
<span class="sd">      * Any other positive value: value indicates reward;</span>
<span class="sd">        Episode will terminate</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">layout</span><span class="p">,</span>
               <span class="n">start_state</span><span class="p">,</span>
               <span class="n">goal_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">,</span>
               <span class="n">discount</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
               <span class="n">penalty_for_walls</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span>
               <span class="n">reward_goal</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
               <span class="n">max_episode_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">randomize_goals</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initiates grid environment</span>

<span class="sd">    Args:</span>
<span class="sd">      layout: List</span>
<span class="sd">        NxN array of numbers, indicating the layout of the environment.</span>
<span class="sd">      start_state: Tuple</span>
<span class="sd">        Tuple (y, x) of starting location.</span>
<span class="sd">      goal_state: Tuple</span>
<span class="sd">        Optional tuple (y, x) of goal location. Will be randomly</span>
<span class="sd">        sampled once if None.</span>
<span class="sd">      observation_type: Enum</span>
<span class="sd">        Enum observation type to use.</span>
<span class="sd">      discount: Float</span>
<span class="sd">        Discounting factor included in all Timesteps.</span>
<span class="sd">      penalty_for_walls: Integer</span>
<span class="sd">        Reward added when hitting a wall (should be negative).</span>
<span class="sd">      reward_goal: Integer</span>
<span class="sd">        Reward added when finding the goal (should be positive).</span>
<span class="sd">      max_episode_length: Integer</span>
<span class="sd">        If set, will terminate an episode after this many steps.</span>
<span class="sd">      randomize_goals: Boolean</span>
<span class="sd">        If true, randomize goal at every episode.</span>

<span class="sd">    Returns:</span>
<span class="sd">      None</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">observation_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ObservationType</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'observation_type should be a ObservationType instace.'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">layout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span> <span class="o">=</span> <span class="n">start_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_states</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_discount</span> <span class="o">=</span> <span class="n">discount</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_penalty_for_walls</span> <span class="o">=</span> <span class="n">penalty_for_walls</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reward_goal</span> <span class="o">=</span> <span class="n">reward_goal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="o">=</span> <span class="n">observation_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_length</span> <span class="o">=</span> <span class="n">max_episode_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_randomize_goals</span> <span class="o">=</span> <span class="n">randomize_goals</span>
    <span class="k">if</span> <span class="n">goal_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Randomly sample goal_state if not provided</span>
      <span class="n">goal_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_goal</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="n">goal_state</span>

  <span class="k">def</span> <span class="nf">_sample_goal</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Randomly sample reachable non-starting state.</span>

<span class="sd">    Args:</span>
<span class="sd">      None</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="c1"># Sample a new goal</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">max_tries</span> <span class="o">=</span> <span class="mf">1e5</span>
    <span class="k">while</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">max_tries</span><span class="p">:</span>
      <span class="n">goal_state</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">goal_state</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">goal_state</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Reachable state found!</span>
        <span class="k">return</span> <span class="n">goal_state</span>
      <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'Failed to sample a goal state.'</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">layout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">number_of_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_states</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">goal_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">start_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>

  <span class="k">def</span> <span class="nf">set_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">action_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">DiscreteArray</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'action'</span><span class="p">)</span>

  <span class="nd">@goal_state</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">goal_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_goal</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">new_goal</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_goal</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'This is not a valid goal!'</span><span class="p">)</span>
    <span class="c1"># Zero out any other goal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Setup new goal location</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_goal</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_goal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span> <span class="o">=</span> <span class="n">new_goal</span>

  <span class="k">def</span> <span class="nf">plot_greedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">greedy_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">plot_policy</span><span class="p">(</span><span class="n">greedy_actions</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">observation_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Function to return the spec-list based on observation type</span>

<span class="sd">    Args:</span>
<span class="sd">      None</span>

<span class="sd">    Returns:</span>
<span class="sd">      Specification-list based on observation type</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_ONEHOT</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'observation_agent_onehot'</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'observation_grid'</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_GOAL_POS</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'observation_agent_goal_pos'</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">DiscreteArray</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_states</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'observation_state_index'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_obs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Returns observation initiating agent state, position, goal state</span>

<span class="sd">    Args:</span>
<span class="sd">      None</span>

<span class="sd">    Returns:</span>
<span class="sd">      Observation</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_ONEHOT</span><span class="p">:</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="c1"># Place agent</span>
      <span class="n">obs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">obs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">:</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">obs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">&lt;</span> <span class="mi">0</span>
      <span class="n">obs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="n">obs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">obs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_GOAL_POS</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">:</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
      <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span>

  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Helper function to reset GridWorld</span>

<span class="sd">    Args:</span>
<span class="sd">      None</span>

<span class="sd">    Returns:</span>
<span class="sd">      Reset environment</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_randomize_goals</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_goal</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">(</span>
        <span class="n">step_type</span><span class="o">=</span><span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">FIRST</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">discount</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_obs</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Helper function to process current position and</span>
<span class="sd">    optimize future steps towards goal</span>

<span class="sd">    Args:</span>
<span class="sd">      action: Integer</span>
<span class="sd">        if 0, move up; if 1, move right; if 2, more down and if 3, move left</span>

<span class="sd">    Returns:</span>
<span class="sd">      Observation from new position;</span>
<span class="sd">    """</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>

    <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Up</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Right</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># Down</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># Left</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">'Invalid action: </span><span class="si">{}</span><span class="s1"> is not 0, 1, 2, or 3.'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">action</span><span class="p">))</span>

    <span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span> <span class="o">=</span> <span class="n">new_state</span>
    <span class="n">step_type</span> <span class="o">=</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">MID</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>  <span class="c1"># Wall</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_penalty_for_walls</span>
      <span class="n">discount</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_discount</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Empty cell</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">discount</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_discount</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Goal</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span><span class="p">]</span>
      <span class="n">discount</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>
      <span class="n">step_type</span> <span class="o">=</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">LAST</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="n">new_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_length</span><span class="p">):</span>
      <span class="n">step_type</span> <span class="o">=</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">LAST</span>
    <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">(</span>
        <span class="n">step_type</span><span class="o">=</span><span class="n">step_type</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">reward</span><span class="p">),</span>
        <span class="n">discount</span><span class="o">=</span><span class="n">discount</span><span class="p">,</span>
        <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_obs</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">plot_grid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">add_start</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Helper function to plot GridWorld</span>

<span class="sd">    Args:</span>
<span class="sd">      add_start: Boolean</span>
<span class="sd">        if True, add start/goal positions</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="c1"># Add start/goal</span>
    <span class="k">if</span> <span class="n">add_start</span><span class="p">:</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
          <span class="sa">r</span><span class="s1">'$\mathbf</span><span class="si">{S}</span><span class="s1">$'</span><span class="p">,</span>
          <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
          <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
          <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="sa">r</span><span class="s1">'$\mathbf</span><span class="si">{G}</span><span class="s1">$'</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
        <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">w</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">'-w'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">h</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">'-w'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">plot_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_rgb</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Helper function to plot agent state</span>

<span class="sd">    Args:</span>
<span class="sd">      return_rgb: Boolean</span>
<span class="sd">        if True, process GridWorld with number-of-channels = 3</span>

<span class="sd">    Returns:</span>
<span class="sd">      data: np.ndarray</span>
<span class="sd">        Array of size (h, w, 3) describing environment</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">(</span><span class="n">add_start</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Add the agent location</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="sa">r</span><span class="s1">'$\mathbf</span><span class="si">{A}</span><span class="s1">$'</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
        <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">return_rgb</span><span class="p">:</span>
      <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'tight'</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">tostring_rgb</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
      <span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">get_width_height</span><span class="p">()</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">data</span>

  <span class="k">def</span> <span class="nf">plot_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Helper function to visualize the policy;</span>

<span class="sd">    Args:</span>
<span class="sd">      policy: PolicyGradientNet instance</span>
<span class="sd">        Describes the principles that govern agent movement</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="n">action_names</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">r</span><span class="s1">'$\uparrow$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$\rightarrow$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$\downarrow$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$\leftarrow$'</span>
    <span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Policy Visualization'</span><span class="p">)</span>
    <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="c1"># if ((y, x) != self._start_state) and ((y, x) != self._goal_state):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">:</span>
          <span class="n">action_name</span> <span class="o">=</span> <span class="n">action_names</span><span class="p">[</span><span class="n">policy</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]]</span>
          <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action_name</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">build_gridworld_task</span><span class="p">(</span><span class="n">task</span><span class="p">,</span>
                         <span class="n">discount</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                         <span class="n">penalty_for_walls</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span>
                         <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">,</span>
                         <span class="n">max_episode_length</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Construct a particular Gridworld layout with start/goal states.</span>

<span class="sd">  Args:</span>
<span class="sd">    task: String</span>
<span class="sd">      String name of the task to use. One of {'simple', 'obstacle',</span>
<span class="sd">        'random_goal'}.</span>
<span class="sd">    discount: Float</span>
<span class="sd">      Discounting factor included in all Timesteps.</span>
<span class="sd">    penalty_for_walls: Integer</span>
<span class="sd">      Reward added when hitting a wall (should be negative).</span>
<span class="sd">    observation_type: Enum O</span>
<span class="sd">      bservation type to use. One of:</span>
<span class="sd">        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.</span>
<span class="sd">        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the</span>
<span class="sd">          agent is and 0 elsewhere.</span>
<span class="sd">        * ObservationType.GRID: NxNx3 float32 grid of feature channels.</span>
<span class="sd">          First channel contains walls (1 if wall, 0 otherwise), second the</span>
<span class="sd">          agent position (1 if agent, 0 otherwise) and third goal position</span>
<span class="sd">          (1 if goal, 0 otherwise)</span>
<span class="sd">        * ObservationType.AGENT_GOAL_POS: float32 tuple with</span>
<span class="sd">          (agent_y, agent_x, goal_y, goal_x).</span>
<span class="sd">    max_episode_length: Integer</span>
<span class="sd">      If set, will terminate an episode after this many steps.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">tasks_specifications</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s1">'simple'</span><span class="p">:</span> <span class="p">{</span>
          <span class="s1">'layout'</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
          <span class="p">],</span>
          <span class="s1">'start_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
          <span class="s1">'goal_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="p">},</span>
      <span class="s1">'obstacle'</span><span class="p">:</span> <span class="p">{</span>
          <span class="s1">'layout'</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
          <span class="p">],</span>
          <span class="s1">'start_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
          <span class="s1">'goal_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
      <span class="p">},</span>
      <span class="s1">'random_goal'</span><span class="p">:</span> <span class="p">{</span>
          <span class="s1">'layout'</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
          <span class="p">],</span>
          <span class="s1">'start_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
          <span class="c1"># 'randomize_goals': True</span>
      <span class="p">},</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">GridWorld</span><span class="p">(</span>
      <span class="n">discount</span><span class="o">=</span><span class="n">discount</span><span class="p">,</span>
      <span class="n">penalty_for_walls</span><span class="o">=</span><span class="n">penalty_for_walls</span><span class="p">,</span>
      <span class="n">observation_type</span><span class="o">=</span><span class="n">observation_type</span><span class="p">,</span>
      <span class="n">max_episode_length</span><span class="o">=</span><span class="n">max_episode_length</span><span class="p">,</span>
      <span class="o">**</span><span class="n">tasks_specifications</span><span class="p">[</span><span class="n">task</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">setup_environment</span><span class="p">(</span><span class="n">environment</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Returns the environment and its spec.</span>

<span class="sd">  Args:</span>
<span class="sd">    environment: acme.wrappers.single_precision.SinglePrecisionWrapper instance</span>
<span class="sd">      Wrapped environment into single-precision floats [avoids floating point errors]</span>

<span class="sd">  Returns:</span>
<span class="sd">    environment: acme.wrappers.single_precision.SinglePrecisionWrapper instance</span>
<span class="sd">      Wrapped environment into single-precision floats [avoids floating point errors]</span>
<span class="sd">    environment_spec: acme.specs.EnvironmentSpec(dm_env.specs.Array instance, dm_env.specs.DiscreteArray instance, dm_env.specs.Array instance, dm_env.specs.BoundedArray instance)</span>
<span class="sd">      Descibes specification of the GridWorld Environment</span>
<span class="sd">  """</span>

  <span class="c1"># Make sure the environment outputs single-precision floats.</span>
  <span class="n">environment</span> <span class="o">=</span> <span class="n">wrappers</span><span class="o">.</span><span class="n">SinglePrecisionWrapper</span><span class="p">(</span><span class="n">environment</span><span class="p">)</span>

  <span class="c1"># Grab the spec of the environment.</span>
  <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">specs</span><span class="o">.</span><span class="n">make_environment_spec</span><span class="p">(</span><span class="n">environment</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span>
</pre></div>
</div>
</div>
</div>
<p>We will use two distinct tabular GridWorlds:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">simple</span></code> where the goal is at the bottom left of the grid, little navigation required.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">obstacle</span></code> where the goal is behind an obstacle the agent must avoid.</p></li>
</ul>
<p>You can visualize the grid worlds by running the cell below.</p>
<p>Note that <strong>S</strong> indicates the start state and <strong>G</strong> indicates the goal.</p>
</div>
</div>
<div class="section" id="visualise-gridworlds">
<h3>Visualise GridWorlds<a class="headerlink" href="#visualise-gridworlds" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Visualise GridWorlds</span>

<span class="c1"># Instantiate two tabular environments, a simple task, and one that involves</span>
<span class="c1"># the avoidance of an obstacle.</span>
<span class="n">simple_grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'simple'</span><span class="p">,</span> <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">)</span>
<span class="n">obstacle_grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'obstacle'</span><span class="p">,</span> <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">)</span>

<span class="c1"># Plot them.</span>
<span class="n">simple_grid</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Simple'</span><span class="p">)</span>

<span class="n">obstacle_grid</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Obstacle'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_29_0.png" src="../../../_images/W3D4_Tutorial5_29_0.png"/>
<img alt="../../../_images/W3D4_Tutorial5_29_1.png" src="../../../_images/W3D4_Tutorial5_29_1.png"/>
</div>
</div>
<p>In this environment, the agent has four possible  <font color="blue"><strong>actions</strong></font>: <code class="docutils literal notranslate"><span class="pre">up</span></code>, <code class="docutils literal notranslate"><span class="pre">right</span></code>, <code class="docutils literal notranslate"><span class="pre">down</span></code>, and <code class="docutils literal notranslate"><span class="pre">left</span></code>.  The <font color="green"><strong>reward</strong></font> is <code class="docutils literal notranslate"><span class="pre">-5</span></code> for bumping into a wall, <code class="docutils literal notranslate"><span class="pre">+10</span></code> for reaching the goal, and <code class="docutils literal notranslate"><span class="pre">0</span></code> otherwise. The episode ends when the agent reaches the goal, and otherwise continues. The <strong>discount</strong> on continuing steps, is <span class="math notranslate nohighlight">\(\gamma = 0.9\)</span>.</p>
<p>Before we start building an agent to interact with this environment, let’s first look at the types of objects the environment either returns (e.g., <font color="redorange"><strong>observations</strong></font>) or consumes (e.g., <font color="blue"><strong>actions</strong></font>). The <code class="docutils literal notranslate"><span class="pre">environment_spec</span></code> will show you the form of the <font color="redorange"><strong>observations</strong></font>, <font color="green"><strong>rewards</strong></font> and <strong>discounts</strong> that the environment exposes and the form of the <font color="blue"><strong>actions</strong></font> that can be taken.</p>
</div>
<div class="section" id="look-at-environment-spec">
<h3>Look at <code class="docutils literal notranslate"><span class="pre">environment_spec</span></code><a class="headerlink" href="#look-at-environment-spec" title="Permalink to this headline">¶</a></h3>
<div class="section" id="note-setup-environment-is-implemented-in-the-same-cell-as-gridworld">
<h4><strong>Note:</strong> <code class="docutils literal notranslate"><span class="pre">setup_environment</span></code> is implemented in the same cell as GridWorld.<a class="headerlink" href="#note-setup-environment-is-implemented-in-the-same-cell-as-gridworld" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Look at `environment_spec`</span>

<span class="c1"># @markdown ##### **Note:** `setup_environment` is implemented in the same cell as GridWorld.</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">simple_grid</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'actions:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'observations:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">observations</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'rewards:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'discounts:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">discounts</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>actions:
 DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=3, num_values=4) 

observations:
 Array(shape=(9, 10, 3), dtype=dtype('float32'), name='observation_grid') 

rewards:
 Array(shape=(), dtype=dtype('float32'), name='reward') 

discounts:
 BoundedArray(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0) 
</pre></div>
</div>
</div>
</div>
<p>We first set the environment to its initial state by calling the <code class="docutils literal notranslate"><span class="pre">reset()</span></code> method which returns the first observation and resets the agent to the starting location.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">environment</span><span class="o">.</span><span class="n">plot_state</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_35_0.png" src="../../../_images/W3D4_Tutorial5_35_0.png"/>
</div>
</div>
<p>Note that <strong>A</strong> indicates the agent’s location.</p>
<p>Now we want to take an action to interact with the environment. We do this by passing a valid action to the <code class="docutils literal notranslate"><span class="pre">dm_env.Environment.step()</span></code> method which returns a <code class="docutils literal notranslate"><span class="pre">dm_env.TimeStep</span></code> namedtuple with fields <code class="docutils literal notranslate"><span class="pre">(step_type,</span> <span class="pre">reward,</span> <span class="pre">discount,</span> <span class="pre">observation)</span></code>.</p>
<p>Let’s take an action and visualise the resulting state of the grid-world. (You’ll need to rerun the cell if you pick a new action.)</p>
<p><strong>Note for kaggle users:</strong> As Kaggle does not render the forms automatically the students should be careful to notice the various instructions and manually play around with the values for the variables</p>
</div>
</div>
<div class="section" id="pick-an-action-and-see-the-state-changing">
<h3>Pick an action and see the state changing<a class="headerlink" href="#pick-an-action-and-see-the-state-changing" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Pick an action and see the state changing</span>
<span class="n">action</span> <span class="o">=</span> <span class="s2">"down"</span> <span class="c1">#@param ["up", "right", "down", "left"] {type:"string"}</span>

<span class="n">action_int</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'up'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
              <span class="s1">'right'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
              <span class="s1">'down'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
              <span class="s1">'left'</span><span class="p">:</span><span class="mi">3</span> <span class="p">}</span>
<span class="n">action</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">action_int</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>
<span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># pytype: dm_env.TimeStep</span>
<span class="n">environment</span><span class="o">.</span><span class="n">plot_state</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_39_0.png" src="../../../_images/W3D4_Tutorial5_39_0.png"/>
</div>
</div>
</div>
<div class="section" id="run-loop">
<h3>Run loop<a class="headerlink" href="#run-loop" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Run loop</span>

<span class="c1"># @markdown ##### This function runs an agent in the environment for a number of episodes, allowing it to learn.</span>

<span class="c1"># @markdown ##### *Double-click* to inspect the `run_loop` function.</span>


<span class="k">def</span> <span class="nf">run_loop</span><span class="p">(</span><span class="n">environment</span><span class="p">,</span>
             <span class="n">agent</span><span class="p">,</span>
             <span class="n">num_episodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">num_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">logger_time_delta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">'training_loop'</span><span class="p">,</span>
             <span class="n">log_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Perform the Acme run loop.</span>
<span class="sd">  Run the environment loop for `num_episodes` episodes. Each episode is itself</span>
<span class="sd">  a loop which interacts first with the environment to get an observation and</span>
<span class="sd">  then give that observation to the agent in order to retrieve an action. Upon</span>
<span class="sd">  termination of an episode a new episode will be started. If the number of</span>
<span class="sd">  episodes is not given then this will interact with the environment</span>
<span class="sd">  infinitely.</span>

<span class="sd">  Args:</span>
<span class="sd">    environment: dm_env</span>
<span class="sd">      Used to generate trajectories.</span>
<span class="sd">    agent: acme.Actor</span>
<span class="sd">      For selecting actions in the run loop.</span>
<span class="sd">    num_steps: Integer</span>
<span class="sd">      Number of steps to run the loop for. If `None` (default), runs</span>
<span class="sd">      without limit.</span>
<span class="sd">    num_episodes: Integer</span>
<span class="sd">      Nmber of episodes to run the loop for. If `None` (default),</span>
<span class="sd">      runs without limit.</span>
<span class="sd">    logger_time_delta: Float</span>
<span class="sd">      Time interval (in seconds) between consecutive logging steps.</span>
<span class="sd">    label: String</span>
<span class="sd">      Optional label used at logging steps.</span>
<span class="sd">    log_loss: Boolean</span>
<span class="sd">      If true, log_loss function is used to compute loss</span>
<span class="sd">      Else, use raw loss function</span>

<span class="sd">  Returns:</span>
<span class="sd">    all_returns: List</span>
<span class="sd">      Log of return per episode</span>
<span class="sd">  """</span>
  <span class="n">logger</span> <span class="o">=</span> <span class="n">loggers</span><span class="o">.</span><span class="n">TerminalLogger</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">time_delta</span><span class="o">=</span><span class="n">logger_time_delta</span><span class="p">)</span>
  <span class="n">iterator</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">)</span> <span class="k">if</span> <span class="n">num_episodes</span> <span class="k">else</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
  <span class="n">all_returns</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">num_total_steps</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
    <span class="c1"># Reset any counts and start the environment.</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">episode_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="c1"># Make the first observation.</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">observe_first</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

    <span class="c1"># Run an episode.</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">timestep</span><span class="o">.</span><span class="n">last</span><span class="p">():</span>
      <span class="c1"># Generate an action from the agent's policy and step the environment.</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">timestep</span><span class="o">.</span><span class="n">observation</span><span class="p">)</span>
      <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

      <span class="c1"># Have the agent observe the timestep and let the agent update itself.</span>
      <span class="n">agent</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">)</span>
      <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

      <span class="c1"># Book-keeping.</span>
      <span class="n">episode_steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">num_total_steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">reward</span>

      <span class="k">if</span> <span class="n">log_loss</span><span class="p">:</span>
        <span class="n">episode_loss</span> <span class="o">+=</span> <span class="n">agent</span><span class="o">.</span><span class="n">last_loss</span>

      <span class="k">if</span> <span class="n">num_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_total_steps</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Collect the results and combine with counts.</span>
    <span class="n">steps_per_second</span> <span class="o">=</span> <span class="n">episode_steps</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">'episode'</span><span class="p">:</span> <span class="n">episode</span><span class="p">,</span>
        <span class="s1">'episode_length'</span><span class="p">:</span> <span class="n">episode_steps</span><span class="p">,</span>
        <span class="s1">'episode_return'</span><span class="p">:</span> <span class="n">episode_return</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">log_loss</span><span class="p">:</span>
      <span class="n">result</span><span class="p">[</span><span class="s1">'loss_avg'</span><span class="p">]</span> <span class="o">=</span> <span class="n">episode_loss</span><span class="o">/</span><span class="n">episode_steps</span>

    <span class="n">all_returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_return</span><span class="p">)</span>

    <span class="c1"># Log the given results.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_total_steps</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
      <span class="k">break</span>
  <span class="k">return</span> <span class="n">all_returns</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implement-the-evaluation-loop">
<h3>Implement the evaluation loop<a class="headerlink" href="#implement-the-evaluation-loop" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Implement the evaluation loop</span>

<span class="c1"># @markdown ##### This function runs the agent in the environment for a number of episodes, without allowing it to learn, in order to evaluate it.</span>

<span class="c1"># @markdown ##### *Double-click* to inspect the `evaluate` function.</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">environment</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">Environment</span><span class="p">,</span>
             <span class="n">agent</span><span class="p">:</span> <span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">,</span>
             <span class="n">evaluation_episodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to run evaluation loop</span>

<span class="sd">  Args:</span>
<span class="sd">    environment: dm_env</span>
<span class="sd">      Used to generate trajectories.</span>
<span class="sd">    agent: acme.Actor</span>
<span class="sd">      For selecting actions in the run loop.</span>
<span class="sd">    evaluation_episodes: Integer</span>
<span class="sd">      Number of episodes for which evaluation loop is to be run for.</span>

<span class="sd">  Returns:</span>
<span class="sd">    frames: List</span>
<span class="sd">      Log of environment state for each time step.</span>
<span class="sd">  """</span>
  <span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluation_episodes</span><span class="p">):</span>
    <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">timestep</span><span class="o">.</span><span class="n">last</span><span class="p">():</span>
      <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">environment</span><span class="o">.</span><span class="n">plot_state</span><span class="p">(</span><span class="n">return_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

      <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">timestep</span><span class="o">.</span><span class="n">observation</span><span class="p">)</span>
      <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">reward</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">'Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s1"> ended with reward </span><span class="si">{</span><span class="n">episode_return</span><span class="si">}</span><span class="s1"> in </span><span class="si">{</span><span class="n">steps</span><span class="si">}</span><span class="s1"> steps'</span>
    <span class="p">)</span>
  <span class="k">return</span> <span class="n">frames</span>


<span class="k">def</span> <span class="nf">display_video</span><span class="p">(</span><span class="n">frames</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
                  <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">'temp.mp4'</span><span class="p">,</span>
                  <span class="n">frame_rate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Save and render video.</span>

<span class="sd">  Args:</span>
<span class="sd">    frames: Sequence[np.ndarray]</span>
<span class="sd">      Log of environment state for each time step.</span>
<span class="sd">    filename: String</span>
<span class="sd">      Name for the video file generated</span>
<span class="sd">    frame_rate: Integer</span>
<span class="sd">      Specifies frequency at which frames are displayed.</span>

<span class="sd">  Returns:</span>
<span class="sd">    IPython.display.HTML(video_tag)</span>
<span class="sd">  """</span>
  <span class="c1"># Write the frames to a video.</span>
  <span class="k">with</span> <span class="n">imageio</span><span class="o">.</span><span class="n">get_writer</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="n">frame_rate</span><span class="p">)</span> <span class="k">as</span> <span class="n">video</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">frames</span><span class="p">:</span>
      <span class="n">video</span><span class="o">.</span><span class="n">append_data</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

  <span class="c1"># Read video and display the video.</span>
  <span class="n">video</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
  <span class="n">b64_video</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
  <span class="n">video_tag</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'&lt;video  width="320" height="240" controls alt="test" '</span>
               <span class="s1">'src="data:video/mp4;base64,</span><span class="si">{0}</span><span class="s1">"&gt;'</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">b64_video</span><span class="o">.</span><span class="n">decode</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="n">video_tag</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-function-approximation">
<h1>Section 1: Function Approximation<a class="headerlink" href="#section-1-function-approximation" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~25mins</em></p>
<div class="section" id="video-1-function-approximation">
<h2>Video 1: Function approximation<a class="headerlink" href="#video-1-function-approximation" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "821540a0b2184828ae2013d668f4588c"}
</script></div>
</div>
<center><img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_BasicReinforcementLearning/static/fm_RL_loop.png" width="500"/></center>
<p>So far we only considered look-up tables for value-functions. In all previous cases every state and action pair <span class="math notranslate nohighlight">\((\color{red}{s}, \color{blue}{a})\)</span>, had an entry in our <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-table. Again, this is possible in this environment as the number of states is equal to the number of cells in the grid. But this is not scalable to situations where, say, the goal location changes or the obstacles are in different locations at every episode (consider how big the table could be in this situation?).</p>
<p>An example (not covered in this tutorial) is ATARI from pixels, where the number of possible frames an agent can see is exponential in the number of pixels on the screen.</p>
<center><img alt="portfolio_view" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_BasicReinforcementLearning/static/atari_pixels.gif" width="200"/></center>
<p>But what we <strong>really</strong> want is just to be able to <em>compute</em> the Q-value, when fed with a particular <span class="math notranslate nohighlight">\((\color{red}{s}, \color{blue}{a})\)</span> pair. So if we had a way to get a function to do this work instead of keeping a big table, we’d get around this problem.</p>
<p>To address this, we can use <strong>function approximation</strong> as a way to generalize Q-values over some representation of the very large state space, and <strong>train</strong> them to output the values they should. In this section, we will explore <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-learning with function approximation, which (although it has been theoretically proven to diverge for some degenerate MDPs) can yield impressive results in very large environments. In particular, we will look at <a class="reference external" href="http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf">Neural Fitted Q (NFQ) Iteration</a> and <a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Deep Q-Networks (DQN)</a>.</p>
</div>
<div class="section" id="section-1-1-replay-buffers">
<h2>Section 1.1 Replay Buffers<a class="headerlink" href="#section-1-1-replay-buffers" title="Permalink to this headline">¶</a></h2>
<p>An important property of off-policy methods like <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-learning is that they involve two policies: one for exploration and one that is being optimized (via the <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-function updates). This means that we can generate data from the <strong>behavior</strong> policy and insert that data into some form of data storage—usually referred to as <strong>replay</strong>.</p>
<p>In order to optimize the <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-function we can then sample data from the replay <font color="purple"><strong>dataset</strong></font> and use that data to perform an update. An illustration of this learning loop is shown below.</p>
<center><img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_BasicReinforcementLearning/static/agent_loop.png" width="400"/></center>
<p>In the next cell we will show how to implement a simple replay buffer. This can be as simple as a python list containing transition data. In more complicated scenarios we might want to have a more performance-tuned variant, we might have to be more concerned about how large replay is and what to do when its full, and we might want to sample from replay in different ways. But a simple python list can go a surprisingly long way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simple replay buffer</span>

<span class="c1"># Create a convenient container for the SARSA tuples required by deep RL agents.</span>
<span class="n">Transitions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">'Transitions'</span><span class="p">,</span> <span class="p">[</span><span class="s1">'state'</span><span class="p">,</span> <span class="s1">'action'</span><span class="p">,</span> <span class="s1">'reward'</span><span class="p">,</span> <span class="s1">'discount'</span><span class="p">,</span> <span class="s1">'next_state'</span><span class="p">])</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  A simple Python Replay Buffer.</span>
<span class="sd">  Queue based implementation.</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">add_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span> <span class="o">=</span> <span class="n">initial_timestep</span><span class="o">.</span><span class="n">observation</span>

  <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="n">transition</span> <span class="o">=</span> <span class="n">Transitions</span><span class="p">(</span>
        <span class="n">state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">timestep</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span>
        <span class="n">discount</span><span class="o">=</span><span class="n">timestep</span><span class="o">.</span><span class="n">discount</span><span class="p">,</span>
        <span class="n">next_state</span><span class="o">=</span><span class="n">timestep</span><span class="o">.</span><span class="n">observation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transition</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span> <span class="o">=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">observation</span>

  <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transitions</span><span class="p">:</span>
    <span class="c1"># Sample a random batch of Transitions as a list.</span>
    <span class="n">batch_as_list</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="c1"># Convert the list of `batch_size` Transitions into a single Transitions</span>
    <span class="c1"># object where each field has `batch_size` stacked fields.</span>
    <span class="k">return</span> <span class="n">tree_utils</span><span class="o">.</span><span class="n">stack_sequence_fields</span><span class="p">(</span><span class="n">batch_as_list</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">flush</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transitions</span><span class="p">:</span>
    <span class="n">entire_buffer</span> <span class="o">=</span> <span class="n">tree_utils</span><span class="o">.</span><span class="n">stack_sequence_fields</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">entire_buffer</span>

  <span class="k">def</span> <span class="nf">is_ready</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-1-2-nfq-agent">
<h2>Section 1.2: NFQ Agent<a class="headerlink" href="#section-1-2-nfq-agent" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf">Neural Fitted Q Iteration</a> was one of the first papers to demonstrate how to leverage recent advances in Deep Learning to approximate the Q-value by a neural network.<span class="math notranslate nohighlight">\(^1\)</span>
In other words, the value <span class="math notranslate nohighlight">\(\color{green}Q(\color{red}{s}, \color{blue}{a})\)</span> are approximated by the output of a neural network <span class="math notranslate nohighlight">\(\color{green}{Q_w}(\color{red}{s}, \color{blue}{a})\)</span> for each possible action <span class="math notranslate nohighlight">\(\color{blue}{a} \in \color{blue}{\mathcal{A}}\)</span>.<span class="math notranslate nohighlight">\(^2\)</span></p>
<p>When introducing function approximations, and neural networks in particular, we need to have a loss to optimize. But looking back at the tabular setting above, you can see that we already have some notion of error: the <strong>TD error</strong>.</p>
<p>By training our neural network to output values such that the <em>TD error is minimized</em>, we will also satisfy the Bellman Optimality Equation, which is a good sufficient condition to enforce, to obtain an optimal policy.
Thanks to automatic differentiation, we can just write the TD error as a loss, e.g., with an <span class="math notranslate nohighlight">\(\ell^2\)</span> loss, but others would work too:</p>
<div class="amsmath math notranslate nohighlight" id="equation-688bd2f8-daa8-4315-bb3a-609a80ab2a17">
<span class="eqno">(121)<a class="headerlink" href="#equation-688bd2f8-daa8-4315-bb3a-609a80ab2a17" title="Permalink to this equation">¶</a></span>\[\begin{equation}
L(\color{green}w) = \mathbb{E}\left[ \left( \color{green}{r} + \gamma \max_\color{blue}{a'} \color{green}{Q_w}(\color{red}{s'}, \color{blue}{a'}) âˆ’ \color{green}{Q_w}(\color{red}{s}, \color{blue}{a})  \right)^2\right].
\end{equation}\]</div>
<p>Then we can compute the gradient with respect to the parameters of the neural network and improve our Q-value approximation incrementally.</p>
<p>NFQ builds on <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-learning, but if one were to update the Q-values online directly, the training can be unstable and very slow.
Instead, NFQ uses a replay buffer, similar to what we see implemented above (Section 6.1), to update the Q-value in a batched setting.</p>
<p>When it was introduced, it also was entirely off-policy using a uniformly random policy to collect data, which was prone to instability when applied to more complex environments (e.g. when the input are pixels or the tasks are longer and more complicated).
But it is a good stepping stone to the more complex agents used today. Here, we will look at a slightly different and modernised implementation of NFQ.</p>
<p>Below you will find an incomplete NFQ agent that takes in observations from a gridworld. Instead of receiving a tabular state, it receives an observation in the form of its (x,y) coordinates in the gridworld, and the (x,y) coordinates of the goal.
<br/></p>
<p>The goal of this coding exercise is to complete this agent by implementing the loss, using mean squared error.</p>
<hr class="docutils"/>
<p><sub><span class="math notranslate nohighlight">\(^1\)</span> If you read the NFQ paper, they use a “control” notation, where there is a “cost to minimize”, instead of “rewards to maximize”, so don’t be surprised if signs/max/min do not correspond.</sub></p>
<p><sub><span class="math notranslate nohighlight">\(^2\)</span> We could feed it <span class="math notranslate nohighlight">\(\color{blue}{a}\)</span> as well and ask <span class="math notranslate nohighlight">\(Q_w\)</span> for a single scalar value, but given we have a fixed number of actions and we usually need to take an <span class="math notranslate nohighlight">\(argmax\)</span> over them, it’s easiest to just output them all in one pass.</sub></p>
<div class="section" id="coding-exercise-1-1-implement-nfq">
<h3>Coding Exercise 1.1: Implement NFQ<a class="headerlink" href="#coding-exercise-1-1-implement-nfq" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a convenient container for the SARS tuples required by NFQ.</span>
<span class="n">Transitions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">'Transitions'</span><span class="p">,</span> <span class="p">[</span><span class="s1">'state'</span><span class="p">,</span> <span class="s1">'action'</span><span class="p">,</span> <span class="s1">'reward'</span><span class="p">,</span> <span class="s1">'discount'</span><span class="p">,</span> <span class="s1">'next_state'</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">NeuralFittedQAgent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Implementation of a Neural Fitted Agent</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">environment_spec</span><span class="p">:</span> <span class="n">specs</span><span class="o">.</span><span class="n">EnvironmentSpec</span><span class="p">,</span>
               <span class="n">q_network</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">replay_capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
               <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
               <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
               <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3e-4</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Neural Fitted Agent Initialisation</span>

<span class="sd">    Args:</span>
<span class="sd">      environment_spec: specs.EnvironmentSpec</span>
<span class="sd">        * actions: DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=3, num_values=4)</span>
<span class="sd">        * observations: Array(shape=(9, 10, 3), dtype=dtype('float32'), name='observation_grid')</span>
<span class="sd">        * rewards: Array(shape=(), dtype=dtype('float32'), name='reward')</span>
<span class="sd">        * discounts: BoundedArray(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)</span>
<span class="sd">      q_network: nn.Module,</span>
<span class="sd">        Q Network</span>
<span class="sd">      replay_capacity: int,</span>
<span class="sd">        Capacity of the replay buffer [default: 100000]</span>
<span class="sd">      epsilon: float</span>
<span class="sd">        Controls the exploration-exploitation tradeoff</span>
<span class="sd">      batch_size: int</span>
<span class="sd">        Batch Size [default = 1]</span>
<span class="sd">      learning_rate: float</span>
<span class="sd">        Rate at which the neural fitted agent learns [default = 3e-4]</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="c1"># Store agent hyperparameters and network.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span> <span class="o">=</span> <span class="n">q_network</span>

    <span class="c1"># Container for the computed loss (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Create the replay buffer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">replay_capacity</span><span class="p">)</span>

    <span class="c1"># Setup optimizer that will train the network to minimize the loss.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Chooses epsilon-greedy action</span>

<span class="sd">    Args:</span>
<span class="sd">      observation: enum</span>
<span class="sd">        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.</span>
<span class="sd">        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the</span>
<span class="sd">          agent is and 0 elsewhere.</span>
<span class="sd">        * ObservationType.GRID: NxNx3 float32 grid of feature channels.</span>
<span class="sd">          First channel contains walls (1 if wall, 0 otherwise), second the</span>
<span class="sd">          agent position (1 if agent, 0 otherwise) and third goal position</span>
<span class="sd">          (1 if goal, 0 otherwise)</span>
<span class="sd">        * ObservationType.AGENT_GOAL_POS: float32 tuple with</span>
<span class="sd">          (agent_y, agent_x, goal_y, goal_x)</span>


<span class="sd">    Returns:</span>
<span class="sd">      action: Integer</span>
<span class="sd">        Chosen action based on epsilon-greedy policy</span>
<span class="sd">    """</span>
    <span class="c1"># Compute Q-values.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># Adds batch dimension.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Removes batch dimension</span>

    <span class="c1"># Select epsilon-greedy action.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">action</span>

  <span class="k">def</span> <span class="nf">q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Updates replay buffer</span>

<span class="sd">    Args:</span>
<span class="sd">      None</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">is_ready</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">):</span>
      <span class="c1"># If the replay buffer is not ready to sample from, do nothing.</span>
      <span class="k">return</span>

    <span class="c1"># Sample a minibatch of transitions from experience replay.</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">)</span>

    <span class="c1"># Note: each of these tensors will be of shape [batch_size, ...].</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">discount</span><span class="p">)</span>
    <span class="n">next_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">next_state</span><span class="p">)</span>

    <span class="c1"># Compute the Q-values at next states in the transitions.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">q_next_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">next_s</span><span class="p">)</span>  <span class="c1"># Shape [batch_size, num_actions].</span>
      <span class="n">max_q_next_s</span> <span class="o">=</span> <span class="n">q_next_s</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="c1"># Compute the TD error and then the losses.</span>
      <span class="n">target_q_value</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">max_q_next_s</span>

    <span class="c1"># Compute the Q-values at original state.</span>
    <span class="n">q_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># Gather the Q-value corresponding to each action in the batch.</span>
    <span class="n">q_s_a</span> <span class="o">=</span> <span class="n">q_s</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete the NFQ Agent"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># TODO Average the squared TD errors over the entire batch using</span>
    <span class="c1"># self._loss_fn, which is defined above as nn.MSELoss()</span>
    <span class="c1"># HINT: Take a look at the reference for nn.MSELoss here:</span>
    <span class="c1">#  https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html</span>
    <span class="c1">#  What should you put for the input and the target?</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Compute the gradients of the loss with respect to the q_network variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Apply the gradient update.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Store the loss for logging purposes (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add_first</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_BasicReinforcementLearning/solutions/W3D4_Tutorial5_Solution_01e4c25b.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="train-and-evaluate-the-nfq-agent">
<h3>Train and Evaluate the NFQ Agent<a class="headerlink" href="#train-and-evaluate-the-nfq-agent" title="Permalink to this headline">¶</a></h3>
<div class="section" id="training-the-nfq-agent">
<h4>Training the NFQ Agent<a class="headerlink" href="#training-the-nfq-agent" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Training the NFQ Agent</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="c1"># @param {type:"number"}</span>

<span class="n">max_episode_length</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Create the environment.</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'simple'</span><span class="p">,</span>
    <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_GOAL_POS</span><span class="p">,</span>
    <span class="n">max_episode_length</span><span class="o">=</span><span class="n">max_episode_length</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="c1"># Define the neural function approximator (aka Q network).</span>
<span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span><span class="p">))</span>

<span class="c1"># Build the trainable Q-learning agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">NeuralFittedQAgent</span><span class="p">(</span>
    <span class="n">environment_spec</span><span class="p">,</span>
    <span class="n">q_network</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
    <span class="n">replay_capacity</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span>
    <span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span>
    <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">logger_time_delta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
    <span class="n">log_loss</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="evaluating-the-agent-set-epsilon-0">
<h4>Evaluating the agent (set <span class="math notranslate nohighlight">\(\epsilon=0\)</span>)<a class="headerlink" href="#evaluating-the-agent-set-epsilon-0" title="Permalink to this headline">¶</a></h4>
<div class="section" id="temporarily-change-epsilon-to-be-more-greedy-remember-to-change-it-back">
<h5>Temporarily change epsilon to be more greedy; remember to change it back.<a class="headerlink" href="#temporarily-change-epsilon-to-be-more-greedy-remember-to-change-it-back" title="Permalink to this headline">¶</a></h5>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Evaluating the agent (set $\epsilon=0$)</span>
<span class="c1"># @markdown ##### Temporarily change epsilon to be more greedy; remember to change it back.</span>

<span class="n">agent</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># Record a few episodes.</span>
<span class="n">frames</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">evaluation_episodes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Change epsilon back.</span>
<span class="n">agent</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

<span class="c1"># Display the video of the episodes.</span>
<span class="n">display_video</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">frame_rate</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Episode 0 ended with reward -990.0 in 200 steps
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Episode 1 ended with reward -990.0 in 200 steps
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Episode 2 ended with reward -990.0 in 200 steps
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Episode 3 ended with reward -990.0 in 200 steps
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Episode 4 ended with reward -990.0 in 200 steps
</pre></div>
</div>
<div class="output text_html"><video alt="test" controls="" height="240" src="data:video/mp4;base64,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" width="320"></video></div></div>
</div>
</div>
</div>
<div class="section" id="visualise-the-learned-q-values">
<h4>Visualise the learned <span class="math notranslate nohighlight">\(Q\)</span> values<a class="headerlink" href="#visualise-the-learned-q-values" title="Permalink to this headline">¶</a></h4>
<div class="section" id="evaluate-the-policy-for-every-state-similar-to-tabular-agents-above">
<h5>Evaluate the policy for every state, similar to tabular agents above.<a class="headerlink" href="#evaluate-the-policy-for-every-state-similar-to-tabular-agents-above" title="Permalink to this headline">¶</a></h5>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Visualise the learned $Q$ values</span>

<span class="c1"># @markdown ##### Evaluate the policy for every state, similar to tabular agents above.</span>

<span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">+</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="c1"># Hack observation to see what the Q-network would output at that point.</span>
    <span class="n">environment</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">get_obs</span><span class="p">()</span>
    <span class="n">q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_values</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>
    <span class="n">pi</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>

<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_62_0.png" src="../../../_images/W3D4_Tutorial5_62_0.png"/>
</div>
</div>
<p>Compare the Q-values approximated with the neural network with the tabular case in <strong>Section 5.3</strong>. Notice how the neural network is generalizing from the visited states to the unvisited similar states, while in the tabular case we updated the value of each state only when we visited that state.</p>
</div>
</div>
</div>
<div class="section" id="compare-the-greedy-and-behaviour-epsilon-greedy-policies">
<h3>Compare the greedy and behaviour (<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy) policies<a class="headerlink" href="#compare-the-greedy-and-behaviour-epsilon-greedy-policies" title="Permalink to this headline">¶</a></h3>
<div class="section" id="compare-the-greedy-policy-with-the-agent-s-policy">
<h4>Compare the greedy policy with the agent’s policy<a class="headerlink" href="#compare-the-greedy-policy-with-the-agent-s-policy" title="Permalink to this headline">¶</a></h4>
<div class="section" id="notice-that-the-agent-s-behavior-policy-has-a-lot-more-randomness-due-to-the-high-epsilon-however-the-greedy-policy-that-s-learned-is-optimal">
<h5>Notice that the agent’s behavior policy has a lot more randomness, due to the high <span class="math notranslate nohighlight">\(\epsilon\)</span>. However, the greedy policy that’s learned is optimal.<a class="headerlink" href="#notice-that-the-agent-s-behavior-policy-has-a-lot-more-randomness-due-to-the-high-epsilon-however-the-greedy-policy-that-s-learned-is-optimal" title="Permalink to this headline">¶</a></h5>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Compare the greedy policy with the agent's policy</span>

<span class="c1"># @markdown ##### Notice that the agent's behavior policy has a lot more randomness, due to the high $\epsilon$. However, the greedy policy that's learned is optimal.</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_greedy_policy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s1">'Greedy policy using the learnt Q-values'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_policy</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s2">"Policy using the agent's behavior policy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_67_0.png" src="../../../_images/W3D4_Tutorial5_67_0.png"/>
<img alt="../../../_images/W3D4_Tutorial5_67_1.png" src="../../../_images/W3D4_Tutorial5_67_1.png"/>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-deep-q-networks-dqn">
<h1>Section 2: Deep Q-Networks (DQN)<a class="headerlink" href="#section-2-deep-q-networks-dqn" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~30mins</em></p>
<div class="section" id="video-2-deep-q-networks-dqn">
<h2>Video 2: Deep Q-Networks (DQN)<a class="headerlink" href="#video-2-deep-q-networks-dqn" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "ddbc6899d7334691babc999e9e64d3dc"}
</script></div>
</div>
<center><img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_BasicReinforcementLearning/static/DQN.jpg" width="500"/><figcaption>Adopted from <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">Mnih et al., 2015</a></figcaption></center>
<p>In this section, we will look at an advanced deep RL Agent based on the following publication, <a class="reference external" href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning">Playing Atari with Deep Reinforcement Learning</a>, which introduced the first deep learning model to successfully learn control policies directly from high-dimensional pixel inputs using RL.</p>
<p>Here the agent will act directly on a pixel representation of the gridworld. You can find an incomplete implementation below.</p>
<div class="section" id="coding-exercise-2-1-run-a-dqn-agent">
<h3>Coding Exercise 2.1: Run a DQN Agent<a class="headerlink" href="#coding-exercise-2-1-run-a-dqn-agent" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Implementation of a Deep Q Network Agent</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">environment_spec</span><span class="p">:</span> <span class="n">specs</span><span class="o">.</span><span class="n">EnvironmentSpec</span><span class="p">,</span>
               <span class="n">network</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">replay_capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
               <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
               <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
               <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
               <span class="n">target_update_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    DQN Based Agent Initialisation</span>

<span class="sd">    Args:</span>
<span class="sd">      environment_spec: specs.EnvironmentSpec</span>
<span class="sd">        * actions: DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=3, num_values=4)</span>
<span class="sd">        * observations: Array(shape=(9, 10, 3), dtype=dtype('float32'), name='observation_grid')</span>
<span class="sd">        * rewards: Array(shape=(), dtype=dtype('float32'), name='reward')</span>
<span class="sd">        * discounts: BoundedArray(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)</span>
<span class="sd">      network: nn.Module</span>
<span class="sd">        Deep Q Network</span>
<span class="sd">      replay_capacity: int</span>
<span class="sd">        Capacity of the replay buffer [default: 100000]</span>
<span class="sd">      epsilon: float</span>
<span class="sd">        Controls the exploration-exploitation tradeoff</span>
<span class="sd">      batch_size: int</span>
<span class="sd">        Batch Size [default = 1]</span>
<span class="sd">      learning_rate: float</span>
<span class="sd">        Rate at which the neural fitted agent learns [default = 3e-4]</span>
<span class="sd">      target_update_frequency: int</span>
<span class="sd">        Frequency with which target network is updated</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="c1"># Store agent hyperparameters and network.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span> <span class="o">=</span> <span class="n">q_network</span>

    <span class="c1"># Create a second q net with the same structure and initial values, which</span>
    <span class="c1"># we'll be updating separately from the learned q-network.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_target_network</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">)</span>

    <span class="c1"># Container for the computed loss (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Create the replay buffer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">replay_capacity</span><span class="p">)</span>
    <span class="c1"># Keep an internal tracker of steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># How often to update the target network</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_target_update_frequency</span> <span class="o">=</span> <span class="n">target_update_frequency</span>
    <span class="c1"># Setup optimizer that will train the network to minimize the loss.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Action Selection Algorithm</span>

<span class="sd">    Args:</span>
<span class="sd">      observation: enum</span>
<span class="sd">        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.</span>
<span class="sd">        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the</span>
<span class="sd">          agent is and 0 elsewhere.</span>
<span class="sd">        * ObservationType.GRID: NxNx3 float32 grid of feature channels.</span>
<span class="sd">          First channel contains walls (1 if wall, 0 otherwise), second the</span>
<span class="sd">          agent position (1 if agent, 0 otherwise) and third goal position</span>
<span class="sd">          (1 if goal, 0 otherwise)</span>
<span class="sd">        * ObservationType.AGENT_GOAL_POS: float32 tuple with</span>
<span class="sd">          (agent_y, agent_x, goal_y, goal_x)</span>

<span class="sd">    Returns:</span>
<span class="sd">      action: Integer</span>
<span class="sd">        Chosen random action</span>
<span class="sd">    """</span>
    <span class="c1"># Compute Q-values.</span>
    <span class="c1"># Sonnet requires a batch dimension, which we squeeze out right after.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># Adds batch dimension.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Removes batch dimension</span>

    <span class="c1"># Select epsilon-greedy action.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">action</span>

  <span class="k">def</span> <span class="nf">q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Updates replay buffer</span>

<span class="sd">    Args:</span>
<span class="sd">      None</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">is_ready</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">):</span>
      <span class="c1"># If the replay buffer is not ready to sample from, do nothing.</span>
      <span class="k">return</span>

    <span class="c1"># Sample a minibatch of transitions from experience replay.</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">)</span>

    <span class="c1"># Optionally unpack the transitions to lighten notation.</span>
    <span class="c1"># Note: each of these tensors will be of shape [batch_size, ...].</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">discount</span><span class="p">)</span>
    <span class="n">next_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">next_state</span><span class="p">)</span>

    <span class="c1"># Compute the Q-values at next states in the transitions.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="c1">#################################################</span>
      <span class="c1"># Fill in missing code below (...),</span>
      <span class="c1"># then remove or comment the line below to test your implementation</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete the DQN Agent"</span><span class="p">)</span>
      <span class="c1">#################################################</span>
      <span class="c1">#TODO get the value of the next states evaluated by the target network</span>
      <span class="c1"># HINT: use self._target_network, defined above.</span>
      <span class="n">q_next_s</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Shape [batch_size, num_actions].</span>

      <span class="n">max_q_next_s</span> <span class="o">=</span> <span class="n">q_next_s</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="c1"># Compute the TD error and then the losses.</span>
      <span class="n">target_q_value</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">max_q_next_s</span>

    <span class="c1"># Compute the Q-values at original state.</span>
    <span class="n">q_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># Gather the Q-value corresponding to each action in the batch.</span>
    <span class="n">q_s_a</span> <span class="o">=</span> <span class="n">q_s</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Average the squared TD errors over the entire batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">target_q_value</span><span class="p">,</span> <span class="n">q_s_a</span><span class="p">)</span>

    <span class="c1"># Compute the gradients of the loss with respect to the q_network variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Apply the gradient update.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_target_network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="c1"># Store the loss for logging purposes (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add_first</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">)</span>


<span class="c1"># Create a convenient container for the SARS tuples required by NFQ.</span>
<span class="n">Transitions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">'Transitions'</span><span class="p">,</span> <span class="p">[</span><span class="s1">'state'</span><span class="p">,</span> <span class="s1">'action'</span><span class="p">,</span> <span class="s1">'reward'</span><span class="p">,</span> <span class="s1">'discount'</span><span class="p">,</span> <span class="s1">'next_state'</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_BasicReinforcementLearning/solutions/W3D4_Tutorial5_Solution_e7b2a488.py"><em>Click for solution</em></a></p>
<div class="section" id="train-and-evaluate-the-dqn-agent">
<h4>Train and evaluate the DQN agent<a class="headerlink" href="#train-and-evaluate-the-dqn-agent" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Train and evaluate the DQN agent</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.25</span>  <span class="c1"># @param {type: "number"}</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># @param {type: "integer"}</span>
<span class="n">max_episode_length</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># @param {type: "integer"}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'simple'</span><span class="p">,</span>
    <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">,</span>
    <span class="n">max_episode_length</span><span class="o">=</span><span class="n">max_episode_length</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Permute</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Build Agent's Network</span>
<span class="sd">  """</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Permute</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">=</span> <span class="n">order</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">order</span><span class="p">)</span>

<span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Permute</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                    <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span><span class="p">)</span>
                          <span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
    <span class="n">environment_spec</span><span class="o">=</span><span class="n">environment_spec</span><span class="p">,</span>
    <span class="n">network</span><span class="o">=</span><span class="n">q_network</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
    <span class="n">target_update_frequency</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span>
    <span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span>
    <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="o">=</span><span class="n">num_episodes</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="id1">
<h4>Visualise the learned <span class="math notranslate nohighlight">\(Q\)</span> values<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id2">
<h5>Evaluate the policy for every state, similar to tabular agents above.<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Visualise the learned $Q$ values</span>
<span class="c1"># @markdown ##### Evaluate the policy for every state, similar to tabular agents above.</span>

<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">+</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="c1"># Hack observation to see what the Q-network would output at that point.</span>
    <span class="n">environment</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">get_obs</span><span class="p">()</span>
    <span class="n">q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_values</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>
    <span class="n">pi</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>

<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_79_0.png" src="../../../_images/W3D4_Tutorial5_79_0.png"/>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h4>Compare the greedy policy with the agent’s policy<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Compare the greedy policy with the agent's policy</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_greedy_policy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s2">"Greedy policy using the learnt Q-values"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_policy</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s2">"Policy using the agent's epsilon-greedy policy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_81_0.png" src="../../../_images/W3D4_Tutorial5_81_0.png"/>
<img alt="../../../_images/W3D4_Tutorial5_81_1.png" src="../../../_images/W3D4_Tutorial5_81_1.png"/>
</div>
</div>
<p><strong>Note:</strong> You get a better estimate of the value functions if you increase <code class="docutils literal notranslate"><span class="pre">num_episodes</span></code> and <code class="docutils literal notranslate"><span class="pre">max_episode_length</span></code>, but this will take longer to train. Feel free to play around after the day!</p>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-learning-the-policy-directly">
<h1>Section 3: Learning the policy directly<a class="headerlink" href="#section-3-learning-the-policy-directly" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~25mins</em></p>
<div class="section" id="video-3-other-deep-rl-methods">
<h2>Video 3: Other Deep RL Methods<a class="headerlink" href="#video-3-other-deep-rl-methods" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "d9c55cb365a04d1682b4476567805e91"}
</script></div>
</div>
</div>
<div class="section" id="cartpole-task">
<h2>Cartpole task<a class="headerlink" href="#cartpole-task" title="Permalink to this headline">¶</a></h2>
<p>Here we switch to training on a different kind of task, which has a continuous action space: Cartpole in <a class="reference external" href="https://gym.openai.com/">Gym</a>. As you recall from the video, policy-based methods are particularly well-suited for these kinds of tasks. We will be exploring two of those methods below.</p>
<br/>
<center><img height="250" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D4_BasicReinforcementLearning/static/cartpole_task.gif"/></center><div class="section" id="make-a-cartpole-environment-gym-make-cartpole-v1">
<h3>Make a CartPole environment, <code class="docutils literal notranslate"><span class="pre">gym.make('CartPole-v1')</span></code><a class="headerlink" href="#make-a-cartpole-environment-gym-make-cartpole-v1" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Make a CartPole environment, `gym.make('CartPole-v1')`</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v1'</span><span class="p">)</span>

<span class="c1"># Set seeds</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-3-1-policy-gradient">
<h2>Section 3.1: Policy gradient<a class="headerlink" href="#section-3-1-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>Now we will turn to policy gradient methods. Rather than defining the policy in terms of a value function, i.e. <span class="math notranslate nohighlight">\(\color{blue}\pi(\color{red}s) = \arg\max_{\color{blue}a}\color{green}Q(\color{red}s, \color{blue}a)\)</span>, we will directly parameterize the policy and write it as the distribution</p>
<div class="amsmath math notranslate nohighlight" id="equation-76bd4957-290b-4968-b70f-55cb7ad8ef53">
<span class="eqno">(122)<a class="headerlink" href="#equation-76bd4957-290b-4968-b70f-55cb7ad8ef53" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{blue}a_t \sim \color{blue}\pi_{\theta}(\color{blue}a_t|\color{red}s_t).
\end{equation}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\theta\)</span> represent the parameters of the policy. We will update the policy parameters using gradient ascent to <strong>maximize</strong> expected future reward.</p>
<p>One convenient way to represent the conditional distribution above is as a function that takes a state <span class="math notranslate nohighlight">\(\color{red}s\)</span> and returns a distribution over actions <span class="math notranslate nohighlight">\(\color{blue}a\)</span>.</p>
<p>Defined below is an agent which implements the REINFORCE algorithm.
REINFORCE (Williams 1992) is the simplest model-free general reinforcement learning technique.</p>
<p>The <strong>basic idea</strong> is to use probabilistic action choice. If the reward at the end turns out to be high, we make <strong>all</strong> actions in this sequence <strong>more likely</strong> (otherwise, we do the opposite).</p>
<p>This strategy could reinforce “bad” actions as well, however they will turn out to be part of trajectories with low reward and will likely not get accentuated.</p>
<p>From the lectures, we know that we need to compute</p>
<div class="amsmath math notranslate nohighlight" id="equation-90241181-78d7-47fa-984f-18ace5cad72e">
<span class="eqno">(123)<a class="headerlink" href="#equation-90241181-78d7-47fa-984f-18ace5cad72e" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\nabla J(\theta) 
= \mathbb{E}
\left[
  \sum_{t=0}^T \color{green} G_t 
  \nabla\log\color{blue}\pi_\theta(\color{red}{s_t})
\right]
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\color{green} G_t\)</span> is the sum over future rewards from time <span class="math notranslate nohighlight">\(t\)</span>, defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-f1f314af-b74e-433a-8640-763f42488ab1">
<span class="eqno">(124)<a class="headerlink" href="#equation-f1f314af-b74e-433a-8640-763f42488ab1" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green} G_t 
= \sum_{n=t}^T \gamma^{n-t} 
\color{green} R(\color{red}{s_t}, \color{blue}{a_t}, \color{red}{s_{t+1}}).
\end{equation}\]</div>
<p>The algorithm below will collect the state, action, and reward data in its buffer until it reaches a full trajectory. It will then update its policy given the above gradient (and the Adam optimizer).</p>
<p>A policy gradient trains an agent without explicitly mapping the value for every state-action pair in an environment by taking small steps and updating the policy based on the reward associated with that step. In this section, we will build a small network that trains using policy gradient using PyTorch.</p>
<p>The agent can receive a reward immediately for an action or it can receive the award at a later time such as the end of the episode.Â</p>
<p>The policy function our agent will try to learn is <span class="math notranslate nohighlight">\(\pi_\theta(a,s)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter vector, <span class="math notranslate nohighlight">\(s\)</span> is a particular state, and <span class="math notranslate nohighlight">\(a\)</span> is an action.</p>
<p>Monte-Carlo Policy Gradient approach will be used, which means the agent will run through an entire episode and then update policy based on the rewards obtained.</p>
<div class="section" id="set-the-hyperparameters-for-policy-gradient">
<h3>Set the hyperparameters for Policy Gradient<a class="headerlink" href="#set-the-hyperparameters-for-policy-gradient" title="Permalink to this headline">¶</a></h3>
<p>Only used in Policy Gradient Method:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set the hyperparameters for Policy Gradient</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="c1"># @param {type:"number"}</span>

<span class="c1"># @markdown Only used in Policy Gradient Method:</span>
<span class="n">hidden_neurons</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># @param {type:"integer"}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-3-1-creating-a-simple-neural-network">
<h3>Coding Exercise 3.1: Creating a simple neural network<a class="headerlink" href="#coding-exercise-3-1-creating-a-simple-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Below you will find some incomplete code. Fill in the missing code to construct the specified neural network.</p>
<p>Let us define a simple feed forward neural network with one hidden layer of 128 neurons and a dropout of 0.6. Let’s use Adam as our optimizer and a learning rate of 0.01. Use the hyperparameters already defined rather than using explicit values.</p>
<p>Using dropout will significantly improve the performance of the policy. Do compare your results with and without dropout and experiment with other hyper-parameter values as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PolicyGradientNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Defines Policy Gradient Network with the following attributes:</span>
<span class="sd">    Feed Forward Network with a single hidden layer</span>
<span class="sd">    width: 128 neurons</span>
<span class="sd">    dropout: 0.6</span>
<span class="sd">    Optimizer: Adam</span>
<span class="sd">    Learning Rate: 0.01</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initiate Policy Gradient Network with above mentioned parameters/hyperparameters</span>

<span class="sd">    Args:</span>
<span class="sd">      None</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PolicyGradientNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="c1">#################################################</span>
    <span class="c1">## TODO for students: Define two linear layers</span>
    <span class="c1">## from the first expression</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: Create FF neural network."</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># HINT: you can construct linear layers using nn.Linear(); what are the</span>
    <span class="c1"># sizes of the inputs and outputs of each of the layers? Also remember</span>
    <span class="c1"># that you need to use hidden_neurons (see hyperparameters section above).</span>
    <span class="c1">#   https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="o">...</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
    <span class="c1"># Episode policy and past rewards</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reward_episode</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Overall reward and past loss</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">past_reward</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">past_loss</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">,</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">,</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_BasicReinforcementLearning/solutions/W3D4_Tutorial5_Solution_7fb6cb36.py"><em>Click for solution</em></a></p>
<p>Now let’s create an instance of the network we have defined and use Adam as the optimizer using the learning_rate as hyperparameter already defined above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span> <span class="o">=</span> <span class="n">PolicyGradientNet</span><span class="p">()</span>
<span class="n">pg_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="select-action">
<h3>Select Action<a class="headerlink" href="#select-action" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">select_action()</span></code> function chooses an action based on our policy probability distribution using the PyTorch distributions package.  Our policy returns a probability for each possible action in our action space (move left or move right) as an array of length two such as [0.7, 0.3].  We then choose an action based on these probabilities, record our history, and return our action.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Select an action (0 or 1) by running policy model and choosing based on the probabilities in state;</span>

<span class="sd">  Args:</span>
<span class="sd">    state: np.ndarray</span>
<span class="sd">      Describes Agent's state</span>

<span class="sd">  Returns:</span>
<span class="sd">    action:</span>
<span class="sd">      Returns chosen action based on policy's probability distribution</span>
<span class="sd">  """</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
  <span class="n">c</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
  <span class="n">action</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

  <span class="c1"># Add log probability of chosen action</span>
  <span class="k">if</span> <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">)])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="update-policy">
<h3>Update policy<a class="headerlink" href="#update-policy" title="Permalink to this headline">¶</a></h3>
<p>This function updates the policy.</p>
<div class="section" id="reward-g-t">
<h4>Reward <span class="math notranslate nohighlight">\(G_t\)</span><a class="headerlink" href="#reward-g-t" title="Permalink to this headline">¶</a></h4>
<p>We update our policy by taking a sample of the action value function <span class="math notranslate nohighlight">\(Q^{\pi_\theta} (s_t,a_t)\)</span> by playing through episodes of the game.  <span class="math notranslate nohighlight">\(Q^{\pi_\theta} (s_t,a_t)\)</span> is defined as the expected return by taking action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> following policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>We know that for every step the simulation continues we receive a reward of 1.  We can use this to calculate the policy gradient at each time step, where <span class="math notranslate nohighlight">\(r\)</span> is the reward for a particular state-action pair.  Rather than using the instantaneous reward, <span class="math notranslate nohighlight">\(r\)</span>, we instead use a long term reward <span class="math notranslate nohighlight">\( v_{t} \)</span> where <span class="math notranslate nohighlight">\(v_t\)</span> is the discounted sum of all future rewards for the length of the episode.   <span class="math notranslate nohighlight">\(v_{t}\)</span> is then,</p>
<div class="amsmath math notranslate nohighlight" id="equation-edba68a5-8e49-4cac-a979-a2564ee816e3">
<span class="eqno">(125)<a class="headerlink" href="#equation-edba68a5-8e49-4cac-a979-a2564ee816e3" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green} G_t 
= \sum_{n=t}^T \gamma^{n-t} 
\color{green} R(\color{red}{s_t}, \color{blue}{a_t}, \color{red}{s_{t+1}}).
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor (0.99).  For example, if an episode lasts 5 steps, the reward for each step will be [4.90, 3.94, 2.97, 1.99, 1].
Next we scale our reward vector by substracting the mean from each element and scaling to unit variance by dividing by the standard deviation.  This practice is common for machine learning applications and the same operation as Scikit Learn’s <strong><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">StandardScaler</a></strong>.  It also has the effect of compensating for future uncertainty.</p>
</div>
<div class="section" id="update-policy-equation">
<h4>Update Policy: equation<a class="headerlink" href="#update-policy-equation" title="Permalink to this headline">¶</a></h4>
<p>After each episode we apply Monte-Carlo Policy Gradient to improve our policy according to the equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9212c32e-e1db-4349-9387-614dad41ec9f">
<span class="eqno">(126)<a class="headerlink" href="#equation-9212c32e-e1db-4349-9387-614dad41ec9f" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\Delta\theta_t = \alpha\nabla_\theta \, \log \pi_\theta (s_t,a_t)G_t
\end{equation}\]</div>
<p>We will then feed our policy history multiplied by our rewards to our optimizer and update the weights of our neural network using stochastic gradient <strong>ascent</strong>.  This should increase the likelihood of actions that got our agent a larger reward.</p>
<p>The following function <code class="docutils literal notranslate"><span class="pre">update_policy</span></code> updates the network weights and therefore the policy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_policy</span><span class="p">():</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to update network weights and policy</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Discount future rewards back to the present using gamma</span>
  <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">policy</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">R</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>

  <span class="c1"># Scale rewards</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards</span> <span class="o">-</span> <span class="n">rewards</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">rewards</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span>
                                          <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

  <span class="c1"># Calculate loss</span>
  <span class="n">pg_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span><span class="p">,</span>
                              <span class="n">Variable</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

  <span class="c1"># Update network weights</span>
  <span class="c1"># Use zero_grad(), backward() and step() methods of the optimizer instance.</span>
  <span class="n">pg_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">pg_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="c1"># Update the weights</span>
  <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
      <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">pg_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

  <span class="c1"># Save and intialize episode past counters</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">past_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pg_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="p">))</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">())</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<p>This is our main policy training loop. For each step in a training episode, we choose an action, take a step through the environment, and record the resulting new state and reward. We call <code class="docutils literal notranslate"><span class="pre">update_policy()</span></code> at the end of each episode to feed the episode history to our neural network and improve our policy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_gradient_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to train policy gradient network</span>

<span class="sd">  Args:</span>
<span class="sd">    episodes: List</span>
<span class="sd">      Log of state per episode</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">running_reward</span> <span class="o">=</span> <span class="mi">10</span>
  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="c1"># Step through environment using chosen action</span>
      <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

      <span class="c1"># Save reward</span>
      <span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Used to determine when the environment is solved.</span>
    <span class="n">running_reward</span> <span class="o">=</span> <span class="p">(</span><span class="n">running_reward</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">time</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">))</span>

    <span class="n">update_policy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="se">\t</span><span class="s2">Last length: </span><span class="si">{</span><span class="n">time</span><span class="si">:</span><span class="s2">5.0f</span><span class="si">}</span><span class="s2">"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">Average length: </span><span class="si">{</span><span class="n">running_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">running_reward</span> <span class="o">&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">reward_threshold</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Solved! Running reward is now </span><span class="si">{</span><span class="n">running_reward</span><span class="si">}</span><span class="s2"> "</span>
            <span class="sa">f</span><span class="s2">"and the last episode runs to </span><span class="si">{</span><span class="n">time</span><span class="si">}</span><span class="s2"> time steps!"</span><span class="p">)</span>
      <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run-the-model">
<h3>Run the model<a class="headerlink" href="#run-the-model" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">episodes</span> <span class="o">=</span> <span class="mi">500</span>   <span class="c1"># @param {type:"integer"}</span>
<span class="n">policy_gradient_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="plot-the-results">
<h3>Plot the results<a class="headerlink" href="#plot-the-results" title="Permalink to this headline">¶</a></h3>
<div class="section" id="plot-the-training-performance-for-policy-gradient">
<h4>Plot the training performance for policy gradient<a class="headerlink" href="#plot-the-training-performance-for-policy-gradient" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plot the training performance for policy gradient</span>

<span class="k">def</span> <span class="nf">plot_policy_gradient_training</span><span class="p">():</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to plot the training performance</span>
<span class="sd">  of policy gradient network</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">window</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">episodes</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>

  <span class="n">fig</span><span class="p">,</span> <span class="p">((</span><span class="n">ax1</span><span class="p">),</span> <span class="p">(</span><span class="n">ax2</span><span class="p">))</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">]);</span>
  <span class="n">rolling_mean</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
  <span class="n">std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rolling_mean</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)),</span>
                   <span class="n">rolling_mean</span><span class="o">-</span><span class="n">std</span><span class="p">,</span> <span class="n">rolling_mean</span><span class="o">+</span><span class="n">std</span><span class="p">,</span>
                   <span class="n">color</span><span class="o">=</span><span class="s1">'orange'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Episode Length Moving Average (</span><span class="si">{</span><span class="n">window</span><span class="si">}</span><span class="s2">-episode window)"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">);</span> <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Episode Length'</span><span class="p">)</span>

  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Episode Length'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Episode Length'</span><span class="p">)</span>

  <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_policy_gradient_training</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
<div class="section" id="exercise-3-1-explore-different-hyperparameters">
<h3>Exercise 3.1: Explore different hyperparameters.<a class="headerlink" href="#exercise-3-1-explore-different-hyperparameters" title="Permalink to this headline">¶</a></h3>
<p>Try running the model again, by modifying the hyperparameters and observe the outputs. Be sure to rerun the function definition cells in order to pick up on the updated values.</p>
<p>What do you see when you</p>
<ol class="simple">
<li><p>increase learning rate</p></li>
<li><p>decrease learning rate</p></li>
<li><p>decrease gamma (<span class="math notranslate nohighlight">\(\gamma\)</span>)</p></li>
<li><p>increase number of hidden neurons in the network</p></li>
</ol>
</div>
</div>
<div class="section" id="section-3-2-actor-critic">
<h2>Section 3.2: Actor-critic<a class="headerlink" href="#section-3-2-actor-critic" title="Permalink to this headline">¶</a></h2>
<p>Recall the policy gradient</p>
<div class="amsmath math notranslate nohighlight" id="equation-ad359ab7-f410-4071-aa5e-cd49c3218cf4">
<span class="eqno">(127)<a class="headerlink" href="#equation-ad359ab7-f410-4071-aa5e-cd49c3218cf4" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\nabla J(\theta) 
= \mathbb{E}
\left[
  \sum_{t=0}^T \color{green} G_t 
  \nabla\log\color{blue}\pi_\theta(\color{red}{s_t})
\right]
\end{equation}\]</div>
<p>The policy parameters are updated using Monte Carlo technique and uses random samples. This introduces high variability in log probabilities and cumulative reward values. This leads to noisy gradients and can cause unstable learning.</p>
<p>One way to reduce variance and increase stability is subtracting the cumulative reward by a baseline:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6446fd00-e9c9-4af4-98b4-fc0e72f1ca1f">
<span class="eqno">(128)<a class="headerlink" href="#equation-6446fd00-e9c9-4af4-98b4-fc0e72f1ca1f" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\nabla J(\theta) 
= \mathbb{E}
\left[
   \sum_{t=0}^T \color{green} (G_t  - b)
  \nabla\log\color{blue}\pi_\theta(\color{red}{s_t})
\right]
\end{equation}\]</div>
<p>Intuitively, reducing cumulative reward will make smaller gradients and thus smaller and more stable (hopefully) updates.</p>
<p>From the lecture slides, we know that in Actor Critic Method:</p>
<ol class="simple">
<li><p>The Critic estimates the value function. This could be the action-value (the Q value) or state-value (the V value).</p></li>
<li><p>The Actor updates the policy distribution in the direction suggested by the Critic (such as with policy gradients).</p></li>
</ol>
<p>Both the Critic and Actor functions are parameterized with neural networks. The “Critic” network parameterizes the Q-value.</p>
<div class="section" id="set-the-hyperparameters-for-actor-critic">
<h3>Set the hyperparameters for Actor Critic<a class="headerlink" href="#set-the-hyperparameters-for-actor-critic" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set the hyperparameters for Actor Critic</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Only used in Actor-Critic Method</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># @param {type:"integer"}</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="actor-critic-network">
<h3>Actor Critic Network<a class="headerlink" href="#actor-critic-network" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ActorCriticNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Build Actor Critic Network</span>
<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Initiate Actor Critic Network</span>

<span class="sd">    Args:</span>
<span class="sd">      num_inputs: int</span>
<span class="sd">        Number of inputs incoming into the Network</span>
<span class="sd">      num_actions: int</span>
<span class="sd">        Number of actions</span>
<span class="sd">      hidden_size: int</span>
<span class="sd">        Size of hidden layer in the network</span>
<span class="sd">      learning_rate: Float</span>
<span class="sd">        Learning rate of Actor Critic Network</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ActorCriticNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">actor_linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">actor_linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">all_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">all_lengths</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">average_lengths</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Describes forward pass of Actor Critic Network</span>

<span class="sd">    Args:</span>
<span class="sd">      state: np.ndarray</span>
<span class="sd">        Describes state</span>

<span class="sd">    Returns:</span>
<span class="sd">      Value and Policy Distribution</span>
<span class="sd">    """</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">critic_linear1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear2</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="n">policy_dist</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actor_linear1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
    <span class="n">policy_dist</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actor_linear2</span><span class="p">(</span><span class="n">policy_dist</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">policy_dist</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3>Training<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">actor_critic_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function to train Actor Critic Network</span>

<span class="sd">  Args:</span>
<span class="sd">    episodes: list</span>
<span class="sd">      Log of episode for all episodes</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">all_lengths</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">average_lengths</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">all_rewards</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">entropy_term</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">steps</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
      <span class="n">value</span><span class="p">,</span> <span class="n">policy_dist</span> <span class="o">=</span> <span class="n">actor_critic</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
      <span class="n">dist</span> <span class="o">=</span> <span class="n">policy_dist</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dist</span><span class="p">))</span>
      <span class="n">log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">policy_dist</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="n">action</span><span class="p">])</span>
      <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">dist</span><span class="p">))</span>
      <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

      <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
      <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
      <span class="n">entropy_term</span> <span class="o">+=</span> <span class="n">entropy</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

      <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">steps</span> <span class="o">==</span> <span class="n">num_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">qval</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">actor_critic</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
        <span class="n">qval</span> <span class="o">=</span> <span class="n">qval</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">all_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
        <span class="n">all_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
        <span class="n">average_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_lengths</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]))</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"episode: </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">,</span><span class="se">\t</span><span class="s2">reward: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="si">}</span><span class="s2">,"</span>
                <span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">total length: </span><span class="si">{</span><span class="n">steps</span><span class="si">}</span><span class="s2">,"</span>
                <span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">average length: </span><span class="si">{</span><span class="n">average_lengths</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">break</span>

    <span class="c1"># compute Q values</span>
    <span class="n">qvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))):</span>
      <span class="n">qval</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">qval</span>
      <span class="n">qvals</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">qval</span>

    <span class="c1">#update actor critic</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="n">qvals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">qvals</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>

    <span class="n">advantage</span> <span class="o">=</span> <span class="n">qvals</span> <span class="o">-</span> <span class="n">values</span>
    <span class="n">actor_loss</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">log_probs</span> <span class="o">*</span> <span class="n">advantage</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">critic_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">advantage</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">ac_loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">critic_loss</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">entropy_term</span>

    <span class="n">ac_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">ac_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">ac_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

  <span class="c1"># Store results</span>
  <span class="n">actor_critic</span><span class="o">.</span><span class="n">average_lengths</span> <span class="o">=</span> <span class="n">average_lengths</span>
  <span class="n">actor_critic</span><span class="o">.</span><span class="n">all_rewards</span> <span class="o">=</span> <span class="n">all_rewards</span>
  <span class="n">actor_critic</span><span class="o">.</span><span class="n">all_lengths</span> <span class="o">=</span> <span class="n">all_lengths</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h3>Run the model<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">episodes</span> <span class="o">=</span> <span class="mi">500</span>   <span class="c1"># @param {type:"integer"}</span>

<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">num_inputs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_outputs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

<span class="n">actor_critic</span> <span class="o">=</span> <span class="n">ActorCriticNet</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">ac_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="n">actor_critic_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 0,	reward: 30.0,	total length: 29,	average length: 29.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 50,	reward: 15.0,	total length: 14,	average length: 17.2
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 100,	reward: 68.0,	total length: 67,	average length: 31.8
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 150,	reward: 27.0,	total length: 26,	average length: 41.5
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 200,	reward: 85.0,	total length: 84,	average length: 107.4
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 250,	reward: 110.0,	total length: 109,	average length: 84.1
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 300,	reward: 126.0,	total length: 125,	average length: 133.4
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 350,	reward: 262.0,	total length: 261,	average length: 155.4
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 400,	reward: 282.0,	total length: 281,	average length: 193.7
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode: 450,	reward: 147.0,	total length: 146,	average length: 231.5
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id6">
<h3>Plot the results<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<div class="section" id="plot-the-training-performance-for-actor-critic">
<h4>Plot the training performance for Actor Critic<a class="headerlink" href="#plot-the-training-performance-for-actor-critic" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plot the training performance for Actor Critic</span>


<span class="k">def</span> <span class="nf">plot_actor_critic_training</span><span class="p">(</span><span class="n">actor_critic</span><span class="p">,</span> <span class="n">episodes</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plot the training performance for Actor Critic</span>

<span class="sd">  Args:</span>
<span class="sd">    actor_critic: nn.module</span>
<span class="sd">      Actor Critic Network whose performance is to be plotted</span>
<span class="sd">    episodes: int</span>
<span class="sd">      Number of episodes</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">window</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">episodes</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

  <span class="n">smoothed_rewards</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">all_rewards</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
  <span class="n">std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">all_rewards</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Smoothed rewards'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">smoothed_rewards</span><span class="p">)),</span>
                   <span class="n">smoothed_rewards</span> <span class="o">-</span> <span class="n">std</span><span class="p">,</span> <span class="n">smoothed_rewards</span> <span class="o">+</span> <span class="n">std</span><span class="p">,</span>
                   <span class="n">color</span><span class="o">=</span><span class="s1">'orange'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Reward'</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">all_lengths</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'All lengths'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">average_lengths</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Average lengths'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Episode length'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="n">plot_actor_critic_training</span><span class="p">(</span><span class="n">actor_critic</span><span class="p">,</span> <span class="n">episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D4_Tutorial5_124_0.png" src="../../../_images/W3D4_Tutorial5_124_0.png"/>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-3-2-1-effect-of-episodes-on-performance">
<h3>Exercise 3.2.1: Effect of episodes on performance<a class="headerlink" href="#exercise-3-2-1-effect-of-episodes-on-performance" title="Permalink to this headline">¶</a></h3>
<p>Change the episodes from 500 to 3000 and observe the performance impact.</p>
</div>
<div class="section" id="exercise-3-2-2-effect-of-learning-rate-on-performance">
<h3>Exercise 3.2.2: Effect of learning rate on performance<a class="headerlink" href="#exercise-3-2-2-effect-of-learning-rate-on-performance" title="Permalink to this headline">¶</a></h3>
<p>Modify the hyperparameters related to learning_rate and gamma and observe the impact on the performance.</p>
<p>Be sure to rerun the function definition cells in order to pick up on the updated values.</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-4-rl-in-the-real-world">
<h1>Section 4: RL in the real world<a class="headerlink" href="#section-4-rl-in-the-real-world" title="Permalink to this headline">¶</a></h1>
<p><em>Time estimate: ~10mins</em></p>
<div class="section" id="video-4-real-world-applications-and-ethics">
<h2>Video 4: Real-world applications and ethics<a class="headerlink" href="#video-4-real-world-applications-and-ethics" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "45e9194a1d654b39ba032665cb59ccb1"}
</script></div>
</div>
</div>
<div class="section" id="exercise-4-group-discussion">
<h2>Exercise 4: Group discussion<a class="headerlink" href="#exercise-4-group-discussion" title="Permalink to this headline">¶</a></h2>
<p>Form a group of 2-3 and have discussions (roughly 3 minutes each) of the following questions:</p>
<ol class="simple">
<li><p><strong>Safety</strong>: what are some safety issues that arise in RL that don’t arise with say, supervised learning?</p></li>
<li><p><strong>Generalization</strong>: What happens if your RL agent is presented with data it hasn’t trained on? (â€œgoes out of distributionâ€)</p></li>
<li><p>How important do you think <strong>interpretability</strong> is in the ethical and safe deployment of RL agents in the real world?</p></li>
</ol>
<p>This should be a very open-ended discussion. Try to have everyone say at least
one thing. They can either take these 3 questions in turn, with 3-4 minutes
allotted to each, or address them all at once, and allow for a more natural
conversation.</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D4_BasicReinforcementLearning/solutions/W3D4_Tutorial5_Solution_5e962725.py"><em>Click for solution</em></a></p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-5-how-to-learn-more">
<h1>Section 5: How to learn more<a class="headerlink" href="#section-5-how-to-learn-more" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-5-how-to-learn-more">
<h2>Video 5: How to learn more<a class="headerlink" href="#video-5-how-to-learn-more" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "08a93ff8938446cb9627dbd000fdb7bc"}
</script></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="appendix-and-further-reading">
<h1>Appendix and further reading<a class="headerlink" href="#appendix-and-further-reading" title="Permalink to this headline">¶</a></h1>
<p>Books and lecture notes</p>
<ul class="simple">
<li><p>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018. url: <a class="reference external" href="http://incompleteideas.net/book/RLbook2018.pdf">incompleteideas.net/book/RLbook2018.pdf</a></p></li>
<li><p>Szepesvári, Csaba. Algorithms for reinforcement learning.” Synthesis lectures on artificial intelligence and machine learning 4.1 (2010): 1-103. url: <a class="reference external" href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf</a></p></li>
</ul>
<p>Lectures and course</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-">RL Course by David Silver</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb">Reinforcement Learning Course | UCL &amp; DeepMind</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford RL Course by Emma Brunskill</a></p></li>
<li><p><a class="reference external" href="https://www.coursera.org/specializations/reinforcement-learning">RL Course on Coursera by Martha White and Adam White</a></p></li>
</ul>
<p>More practical:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spinningup.openai.com/en/latest/">Spinning Up in Deep RL by Josh Achiam</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2006.00979">Acme white paper</a> &amp; <a class="reference external" href="https://github.com/deepmind/acme/blob/master/examples/tutorial.ipynb">Colab tutorial</a></p></li>
</ul>
<br/>
<p>Link to the <a class="reference external" href="https://twitter.com/FeryalMP/status/1407272291579355136?s=20">tweet thread</a> with resources recommended by the community.</p>
<br/>
<p>This notebook is based on the <a class="reference external" href="https://colab.research.google.com/github/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb">EEML 2020 RL practical</a> by Feryal Behbahani &amp; Gheorghe Comanici. If you are interested in JAX you should try the colab. If you are interested in Tensorflow, there is also a version of the colab for the <a class="reference external" href="https://github.com/Feryal/rl_mlss_2020">MLSS 2020 RL Tutorial</a> that you can try!</p>
</div>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {"e64f60cf22f546639f49d68130af59a4": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1d5fe2de23ac475b9fb18a90011be3c1": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_e64f60cf22f546639f49d68130af59a4", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1sg411M7cn\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f915879ee90>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1sg411M7cn&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "ed5cb7a4c84043d0bd7c3a315803d380": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3523b3b9401141468dfd09aa9327e27e": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_ed5cb7a4c84043d0bd7c3a315803d380", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=7_MYePyYhrM\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f9158768c50>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/7_MYePyYhrM?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "e5bd78d47fcc4a97904bbba21d08b4ba": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "821540a0b2184828ae2013d668f4588c": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_3523b3b9401141468dfd09aa9327e27e", "IPY_MODEL_1d5fe2de23ac475b9fb18a90011be3c1"], "layout": "IPY_MODEL_e5bd78d47fcc4a97904bbba21d08b4ba", "selected_index": 0}}, "9c374b4a148e401c9f2a3f3dd93b1c0d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1f0f6d2964ae4c44a27cb14b0ab2b199": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_9c374b4a148e401c9f2a3f3dd93b1c0d", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1Mo4y1Q7yD\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f9129369550>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1Mo4y1Q7yD&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "a62ef426d08c452da57239bc20638a3b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0d9467a37a94461ba1b060a1e400db29": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_a62ef426d08c452da57239bc20638a3b", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=HEDoNtV1y-w\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f91587d6290>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/HEDoNtV1y-w?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "53cd9e89fe2349cebeedb4f21baa34e9": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ddbc6899d7334691babc999e9e64d3dc": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_0d9467a37a94461ba1b060a1e400db29", "IPY_MODEL_1f0f6d2964ae4c44a27cb14b0ab2b199"], "layout": "IPY_MODEL_53cd9e89fe2349cebeedb4f21baa34e9", "selected_index": 0}}, "5164542b7aa44368ae0ead4619a2a8c1": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "99738e62d3b2459c9f738ae8e77b4e0d": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_5164542b7aa44368ae0ead4619a2a8c1", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV14w411977Y\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f9128f25610>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV14w411977Y&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "2203fece58794082bc6d4d2e9c8132c3": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a3b0ab5611a848038cc3bd3b8df2bdf5": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_2203fece58794082bc6d4d2e9c8132c3", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=1N4Jm9loJx4\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f91300efd90>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/1N4Jm9loJx4?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "19d4a9ed3b4e4f64a7dacc2ed25b3ff2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d9c55cb365a04d1682b4476567805e91": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_a3b0ab5611a848038cc3bd3b8df2bdf5", "IPY_MODEL_99738e62d3b2459c9f738ae8e77b4e0d"], "layout": "IPY_MODEL_19d4a9ed3b4e4f64a7dacc2ed25b3ff2", "selected_index": 0}}, "71c463a42f8f4facab5e2bd9744a3877": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "697c5b1cf70e48088f9f0c1d6ebf64bb": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_71c463a42f8f4facab5e2bd9744a3877", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1Nq4y1X7AF\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f9128074f50>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1Nq4y1X7AF&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "7a4fb1c63de448f2860ce0d159075454": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "abe4d5bd81a8486e9df18be87b47ee64": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_7a4fb1c63de448f2860ce0d159075454", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=5kBtiW88QVw\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f9128134dd0>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/5kBtiW88QVw?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDBoYFhsaGRoeHRsfIyomIiIiIjIrMSguMjc1MjI3LzI3QlBCNThLPS0tRWFFS1NWW1xbNUFlbWRYbFBZW1cBERISGRYZMBsbMFc/NUJXV1dXV1dXV1dXWldXXVdXV1dXV1deV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXXf/AABEIAWgB4AMBIgACEQEDEQH/xAAbAAEAAgMBAQAAAAAAAAAAAAAABAUBAwYHAv/EAEoQAAEDAgMEBgYGBwYGAgMAAAEAAgMEERIhMQUTQVEGImFxktIUFzJTgZEjQqGxwdEHFTNScuHwVGJzk7LCFiRDgqLxNIMlRHT/xAAYAQEBAQEBAAAAAAAAAAAAAAAAAQIDBP/EACwRAQEAAgICAQIFAgcAAAAAAAABAhEhMQMSQVFxEyJhgfAyoQQjQpHB0eH/2gAMAwEAAhEDEQA/APP0REBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEXX+rit97T+J/kT1cVvvafxP8iDkEV3W9F54JDG98RItoTbPPkortjSDVzPmfyU2K5FMOzXji37fyWBs5/NvzVERFN/VUtiQAba2K+DQScvvQRUV7/wAJ1FrmSEA83Efgtb+jkjdZqf8AzP5KbHsyIsKjKKum21Cx72WlcWGzsEL3AGwOoFtCFLpalk0bZI3BzHaEIurG5FhZRBFppqlkrS5huA5zTlbNpLT9oK2oMosIgyiwtVPUtkxYDfC4sOVsxqg3ItL6lgkZET13hzmi3Btr5/8AcFrrq+Ona10mLrOwtDWlxJIJtYZ8Ci6qUig0m1YZn7tpc19r4XscwkcwHAXU5CyzsRamVDXPewG7mWxCxyvmFtRBFEi2hG+V0TcZLbhzgx2EEajFa189FLRbNCLCIjKLCIMoiICIviWVrGl7yGtaLknQAc0H2iwDcXC001UyUOLDcNc5hytm02KDeiwsoCIsIMotc8zY2Oe82a0EuNr2A7AvprgQCNDmEH0iwiDKLCrJukFMx7mOe7E0kHqHUfBaxxyy6m0tk7WiLTS1LZY2yMN2u0Nrdi2rNmlZRFhBlEWEGUWhtWwyuiB67QCRbge1bks0MosIgyiIg4vpE3/m5O5v3KjqW2Cvtvt/5uT/ALfuCqJ3NABeCRfMBY+WlDVP6wAFyt2xKZs03W9luZXztFzTI4x3DQAbWU3orExxe517g9WxI+5LeFxm66Cv2Qx0d47MeB1bG17f1qtWxanfwyNcBjju12Wt+P3/ACVhStbI2MkvBaOBLf8A2qTYkzRXVTW+y9zr/Am34rOPLec0dIY8oiP3FQOae1dTt5n0cP8AD+S557DbioTp66sLKLs4uZNdPTyV8kcLZGNlBcTIQR1GXyDTcAZrAEkLaeEOc/0h0s0hgsL6OwsJIsOtre9gughpWMdI5ozkdifne5sB9wCj/qiDdNiwkMa7EyziCw/3TqNSs6dZnEGKokgbUOeJ2U7Y8TXSFrnNdmCGm5vwIvxWunMsVTTC07RKXNeJZA8GzC4HXquuOGWqtY9mxBkjHYpBILP3jy4kcs9BmdF8xbKja6N95HOjvgL5HOtcFvE8imk9ogbIqXR07rRSS3nn9i2X0jtbkL5qqySN9RHicHTNjdBfVpf9GQP4SA74q5pqZkTS1gsC5zjnfNxLj9pK+J6GOSWOV7bvivgPK+R71dHtNqqmqpHvp4MbscRkMxGrt31W3/ixNd8FpiEklCa01EjZiwyizuo21zhw6EZWN8+1XcVFGyWSZrbSSBoeeeHRRn7DgLiSH4S7EY8bsBOtyy9tc1NHtEaNz6iqaDJJGz0eOQsabdYl2vH81F3rwx0bHmPfVr2OeNWjU25E2tftV+KZglMoHXLQwm/AEkZfErS/ZkLo3xlt2veXnM3xE3uDqDfkmiZxAFLuq+nAfI5pimsHuxWzjvYnPNfXSMuHohY0Of6S2wJsD1X6mxt8lLp9lRRyCUY3SAFuJ73ONjbLM9i31FKyQsLxfdvD252s4Aj8Sro9puVRsnlmqXSSsZG6ja8iNri4vLm5G9h1bfb3LVBJUOgZOxtU+dwa++Ju7dexLcOKwbbIG110DqNhmbNb6RrS24OrTwPMKOzY0LSMO8DQ7EIxI4MBvf2b21ztoppfeK/alVI1m0S17gY2Rllj7JLc7clumhdBU01ppXb5z2SBzrg9QuBA0abt4W1U+fZ0UgmDmkiYASZnO2Q7ltmpmPdG9wu6Mlzc9CQW/cSrpn2jm6cugpqh0cjw51W6LE52LCDIG4rH61jrzVjLG6lnpsEsj2yvMb2yPLr9VzsQvoRh4ZZqeNnxbuSPACyRznPBzuXG5Wul2VFE8SDG94FmmR7n4Rybc5KaaucqpiqJY6SpqjI974zOGNceq0B5Ay42tx4Kzp6AxWk38z3AEuu+4fl+7oOzDZSoaSNjHMa3qOLiQc74iS7XvKj0uyIonNc3edT2GukcWs4dUE2GSukuUqkp5qiWnbOxtS6oe3G0hzd3nmG4cVsPDS/xVg6N09ZNG6SVjGwxuwMfh6xL87jPgpQ2NCHEjeNaXYixsjgy+vsg214aKU2mYJXSgdd7WtJvwbe2XxKaLnPhz8LpfRqSoM8rpHSxMdd3VLXOwkFummd9brYZnzzVGJtSRHJu2CF4aG2ANz1hcm988rWVuNnRCKOLCcEbmuaLnItOIZ96+KjZcb3mS8jHuFnGORzMVtL2OffqppfeK/FUP9CZK+SJ7t4JMJALg0ZXtcC+Ry0uou0IyINow7yRzI4w9mJ5JF2kkE6ltxoV0AoowYiB+yBDMzlcW+OXNfMlBE4y4m33zQ2TM5gAj4ZEppJnN/z6vuiiwRMaC52Wrjc/MqB0d/Zzf/0zf6yrCkphEwMaXEDi9xcfmVBOwYLuIMzcTi4hsz2i5NybA2VTc5QpJ3zVFQ1zagticGMELg0Dqh1z1gSTfusF9OnlMMEUom3z3vGFjmsc9rb5vcPZywk2Oqsp9lxvfju9j7BpdG9zS4DTFY5pLsyJzI29Zu6zY5ryHN4Hram/G+qml9opXVErKeuYHyNMRZgxuxObiDSRivmL3t3qbWQPY+libPN9JI7G7FmeoTbSwGXAZKYNkw4ZWkOIltvCXkl1shmT2KRLTNe+N7h1oyS3PQkEHvyJTRc5/PsoqiR8A2hGyWQtjgbIwucXFpIfezjn9UFScDpqoRulkaz0dji1jsNySRqMx8FYTbPieZS5pO+YI35nNovYdntFfbKVjZN4B1sAZe/1RmPvV0e0UTaiXcthErhiq3Qb36wYC468yBhv2qW2J0NbDG2WUxuikOF78WYLM7nPjxW+s2e3cyMZEJA9+NzS8tuSbktdwPEafBRdnbNd6SJnRvja2NzPpJd49xcRqbmwFshfiVF3KtqWDdxtYXvfhHtPN3HvK56hFRv6vcGK29OLeX5m1rLoaWnbFG2Nl8LdMRJPzOagzdH6Z73PcwlziSesdT8V38Wcx3L8/u4Zy28I+05poIoZi9u8a7C6NpOF976DmtTaoijbI6aRz5ni27sSCT7Db6K0i2VCwxkNP0V8FySBfXLnmvl2x4C1zMJwudjsCcncxy+C3M8NaZ9araGeVk8sRdLh3JeBKQXA9hHBRBJOKann9IkLnvDbHSxJ4cdOKv4dlxMcXgOLi3CS5xJI+JWf1XDumRYTgY4OaLnIi/H4q/i473r6J61WMdJFUzRb5727gvBccwezkohfM2iZV+kyF4t1Seqeta1uJV/UUTXOfIB9K6Msvfh3Kv2dsCIRxmZn0jdRiyJvxGhysrj5Mdbv6fH3LjWiSqdHU1UoHWFO1wB55LXSSVP0UjPSHlxBfiw4HNOuEXyV8KOPePkw9Z7cLrnIju0UeDYsEbw9rXAtN2jEbA9gupPLjrr+aX1qqrJJo5ZHzOqGxh/UfEQWNbfLE1dG03AIzBUCTYlO5xcWnrHE5uI4SeZF7KwAXPyZY5SaaxljKIi5NPP+kNU8bUkZlu8LfnhuoNS7qlWu3Nj1E20pJGBrWWZ1naHq2yAzKl0mxooiHSF0j+ZyaO5v53WariayJ/WcGEMOQJBF7cua17LqDG8NzwvyNuC7XaNHFM4k4yQMwXE31y/kuLoaZ2/aC1wAOfVKtnC43l1M9Uaale+M4jbK97XOXFV3RSmDq1jQ4lpaXF3eMx81MrqZ0zmwta7Da7jb+s/zX1sOifTMleQQ4HI9guphi15MuVn0moHxwsLes1mRcOGlrrkXuJ5rvNlba3gs8Gx1uOfBQttdGDITJSutfMxnT/tPDuUyx+hjl8OtREW3MREQFhZRAREQEWuKdj74HtdY2OEg2PbZfaDKIsIMovljw4BzSCCLgg3BHYheAQCRc3sL5m2tkH0iIgIiICIiAiIgIiICIiAsLKIMLKIgIiICIiAiIgwiyiAiIgIiICIiDCLDnAWuQLmwvzRrgdCDnbLmNUH0iIgIiICIviSVrPacG35m3C/3An4IPtFgG+YWHvDRdxAHMm2uSD6REQEREBYWUQEREBERARFzrdqyxxl7Wtke51Q9+N5bhERIwgWPAAfag6JaK6AywSxtdgc9jmh37pIIB+CpJukT2BwdHE1zXWu6Qhv7MSAA4faOK1rcCexfU/SCRjXvMLWsEojBc+1iWh5L7gBozA11+0NQ2fVNLjFDHEN01mFrm6tcL4S0NNrYrXOp+qvuPZ1aQ3eSuBG6HVlIFg9287zgLRc5/HNb6/bjoYYJN21plYXkSPsAQAcAcAbuN8u4rbTbVe+cMdG1rHPewHEcV2tDsxa3Ma8EEVlBWh8V5Tga4/XzDRI49b968eEZg6cNV9TRVL6yTAXhjXx2JeQ0NsC8YPrXGV+B5Wz0Ha1SX2G6LmyztLMRHVa0luLIkG1j8VlnSIg3wjC+Vres8XaCyJwDQBc/tDzN+w5B802y6xu4jL93E2Jsby2Q5DdkGw0uH2N7cNeCzFBWzwMlxFssjHmwdbBcNDMNxlk0nvKlbR2vJBUStIjMbYWvaCSCSXFriTb2W5E8h3rS7pDLgLmRRvwtc4neGzg1+C7DhNwb3v8Afqgj7RbURzRR45t0XuILXOc7DijsLgi5zfqTYE5Gwt1CoJ+kMkZDXRx4g54PXIDi1zQGx5ZuIde3Z8Rti2xM5zQIWWdJM0dc3wxOwk2w+0bZD7UF2i5uPpK9zMQiY4ktaMDy4Y3glrCbZEEAHlcHsW+n2m6Z9LK0FokkfHhuc2BhJcRwIey1+3tQXqIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIi479I8744KcxvewmQ3LXEcOxB2KLxMVlRYH0qX/Md+aNrKguw+lSDtMrgPmi2WPX9pwPe2N0bQ50cjX4SbXAuCL87FVkGzqpj4nNIAMkj3txnC3G9zjkLXIDhzBtoNV5oauosT6W82Om+dc93YsmqqLgelvz4751h3oj0ij2bW2YJpn2xt3mGTMgNfiI4gEluWWmgW2pjqX1r92XhjTCQcZDQLnH1bWdcC3Z2LzP0mo/tjv8AOcsGrqBb/m3m5tlM7LvQek02zq3SWZ2b2Yy19rgYsRbrhBu0Wy7ha5+jR1l5hdxYXtIG9Ic5uJ1wHaNFi3gNCP7x8zkrKho/+W938Mzitf6xqP7RN/mO/NB6jFQ1gLHOlJc0w5Y+rYOO9uLZnCQL8SL5KG3Zde7OQg2OJoL72cY5WuIJvqXM7OwLzr9Y1H9om/zHfmn6xqP7RN/mO/NB6c6iq7zEYxiwBo3xthBGIgXBDjnoQLADmviPZdX7bzeR0UTXHeGwLH3OR1u3j3815p+saj+0Tf5jvzT9Y1H9om/zHfmg9PFDWkuBkcAXtxHee0N4CS0fU+juLflc2uzYpGRYZSXODn2JNzhxHDc8ThsvG/1jUf2ib/Md+a2tq6g4P+beMZsbyuGHTN3IZ6oPakXjgFUYpJRWEsY/Afp3XOmYF7kZ6q//AEd1UslZKJJZHgQk2c8nPE3mg9EREQEREBERBhfDImtLi1oBcbusNTpc9uQWxEGqdt2myqoNqWdherlcrtiLBMeRzUyuosdNFM14yK2Lj6aucw5HJXtHtVrxY6qTLZpZrAIPLJRairAaXE2aP6+K5in6Qskmdu3OY+/suFr/AAP3arSOySyraTbDHZSdV3PgfyVi51mki2l89EGUUOgc/MOkbJ2g9/D5fJTUGF8mJpeH4RjAIDrZgGxIHfYfJfaICIiAiIgIioOklHNK5hiaSGxSXIJuCSyxbYjr2xEX5IL9FzVX6c8TMwyObZ1hhbYWe3d4DkSS25IPHkszTbQDOoJCAZMDiyPG4gM3YkGjWkmQEixsBog6RFzro61r3lofI4TSOZjw4Q0xOw4SLZYrD/2vp81aI4yzfuIk62KONrnCwyNrgC+LOw01GRIX91lV5aX1oJBwwxZHgXSHh2gM/wDJUjNnVmFjSX7sVAlti62cti0/3A27vjbgg6pZXN08u0XvLXY42Oe3rFrCWA7zEBlbK0f72upXxJUVbHRY8bHSuY1zmRtLjaOUusDcXu1pQdOsKhbUVpDWvjdiduySGtsBY4wc8je3zyUaIV9mRNEjGbkMv1cnbu4cDa98eWvw4oOoWFzY9MbYjegFsIfJgYZMhJisDkTiwDTQ/FSKV9eXxuluBijD4w1uGxju831yflrz1QXqIiAiIgLiv0mfsKf/ABD/AKV2q4r9Jn7Cn/xD/pQcJHShzQd9E2/BzjcfYvo0Wf7aHxHyqZQMa6MYpWs11spNDTtllALyI72c+2ncOK7TxzW7TlVihv8A9eHxO8qi4TyK62rpooKiLcSYutfHpawudVbtfJJUWkiabjINJabXzOIZHLu0yXnzymN4ak43XneE8j8lhejvnMDJXMdGwZ+0DibraxN73yXB7yIkhxkLs72A5/NTDL2q3HXKJZLK22XUQRVLHdcGxzdaw6q+JJm1FQ4tGp42Hd3LpeEwx9rIrLJZXjtnYS03sToMTfzU6Cja3iHA9ov9ixt1vixls25bCeR+SYTyPyXoLHNaG87DmtcswbIDnZw5ZZLyz/FW3Ux/ut8Ek3twWE8j8kwnkfkuwlqCDTlpc0kueCRxHG1jwcdVpm2q6nYCXXuxzWg2PWGbSbAZdZ9+9ejDye3w5ZY6cqWHkfkuu/RqP+dm/wAH/c1bNm7Y9IlewXBEYc3F2e1meyx+B5qb0Gjw1Dza14v9zVvbNjuERFUEREBERB8PdZanVFltkbdRJY7A30Vg+/TmDUqm6RTRvDHNcC4ZEL4rqgC4bn2rlK3aLQ+2LE6/DgpRP3ueS2xzEKsbMt7JfkuTS8hrb5PzHNaa/Y8NQMQFncHDUKDHIpUNQRobKzLRpWufU0htIDNF+8NR/XarvZO3MrxPD28Wnh+S3R1DZBZwsVTba2KImmeElj2i/V0K3LL0jt6PaEcuQNnfun8Oalry+g6QaNmyPB4/EcF1+z9tkAYjvGHRwOf81UdCi1QVDJG3Y4Efd3ragIiICIiAsLKICIiAsLXUTsiYXyODWt1JXOVm3J5Qdw0xRe8cLud3DQI3jhcufh0skjWi7nBo5k2UU7XpRrUQ/wCY381zJ2DLL1343k53e9bo+i7uJYPiSquvHPm10sNbDJ+zljf/AAvB+5by0ZZaaLits7CbT07psQLmkaNtqba3W/ZUlSyjiqIpsYI60UmYyJBwnUaIeuF6v+7r0Vds3a7JyWEGOZvtRu17xzCsFGMsbjdVlEREEREBERAXFfpN/YU/+If9K7Vcf+kWIPhpwTb6Q/6UHnGIjjZbIqhzNFedHtmzVhkiEzmxx2Dm4rXDr5DI9qrduUDKeYsieXgZEOGbTyNsj3jmnvetrp8HacmJjupdgIALbjPmCpn/ABJPiDi2IkC18HD5qBBTg4N6XMBOZtwzt9oVmdjwG2Cckk2F2jszOeWqlkvZutU3SSofmd3f+Hv7eF1W085jlEgDSQSbOFxmpFTQmO4OZDha3EG9rDXgr2Do7C6OIlkoc9gc7raX+5Z4x6i81zG8+k3lhfFita4v3clmOYteXgAE8LZDuXQDo/AXtZvrOJI4kdijs2RFumPcXgkuBtnexystye10m7h+ZUvqnkk3tfkttPtCSO9iDfnmrql2DA9j3Evy0u61zryPJYm2PS7rHG57je18Y15EcNQt/hXfqn4t/qRG9JqgAC0WX90/mjuk1QRYtiPe0/mtEGzg8ZYnHPIFfTdivMjWmzQ4XFzn8AvN+H45d6jpc8/q2u6TTuZIx0cLmvbhILDkOzPIrJ6TzFmF8NM/Jt3OjN3YdCbG11Fds0tcWkOdbkp0nR4uiL4rktGbSbl3cusxknHTncudVrf0mlcbmClvhc2+7Ojtfrdiu/0eVb5KuQOtlDw/iaq2l6OtdT43h7ZCCRc27rhWP6OosNXL2w/7mq60kyleiIiIoiIgIiINU82Adq5zbe3Iof2j7u4Mbmflw7yulkjDmlp0Pw+R4Lg9u9CJGl0lK4ygm5Y49f4E+18c+9BQbR21LPcewz90HXvKrV9vjLXFrgQ4ZEEWI7wvlBYxSXAK3teq+mdlZSGvWLOVlT45VJZMqwOW1kiz0q5ikVvGBNC5h5LmYaiyutl1NnBMbrIvTg5oyx7mHVpI+WS20lbJCbsdYcQdD8FY9K6XdVr7aPAePj/6VMurLrdl7ea5w6xik78j3H8CurottA2bKLH94afHkvPdj9HKmssWNwRcZH5D4DV3wy7V6NsfYcdIwNxOkcPrvz+Q4BBZrKIgIiICIiAvmR4a0ucbNAuSeAX0uc6ZVpbCyBvtSnPO3VHb2kj7Uawx9rpQ7R6StlqQ58ZfHGfo4ybD+J3M/crBu0mVMDntBacebTw/NcnHI0SOD8LTzAFrq0oSxz2Mhc5ziOsCdcjmB32WtLnn7ddO+pj9Gz+ELaqs1zoo23jOQAve/wByiHbWM4CcLTqQSD8xosZXUtYTekTC6jlAF8h9hBVF0cmJoJI3ZGN5sOw5/mviurmYG4JXveJSC0SOJLRiyNza2ipdpzOuX4HtvbC63dxC5ePze/xr7t3CzH2XlXTul2i1rXljsALXDgQLhdLsjaBma5kgwzxHDI37iOwrzig2lJFM2QPuQRm7PLiM+xdvVTbuSnrW5NeGsl/hdofgV2l27XC69Ld/T+fq6FFhZR5hERAREQFxn6SXkQU9jb6Q/wCldmuP/SNAXwQWIykOvcg43Y21RSuc8F5c/wBoC1jbRfUu04nVAnAdiN8YcAQ4Ead+gXy3ZwEbHOjab5FwmOX8QsbfBb9p7H9FMQfE128FwWTkggWv9XtCmp2s30jbSro5cIZcBoyOEDPW2Wg/ErdsHbYppPpI2yMPE6t7ueixBsreNNoLW4mY692FRZtlSMbiJZbkCb/cpuLqx91W1C+d7wbMcTo0AkWsLka8FZx7eibTMjBOPA1pNjYZaBQz0Yqcv2eYv7Wn2LW/o9OBe8Z7A4/klk6JucrWTalGIAxkj8RN3HdnrZaZ5DPvWifadLuY2xueXBzi7E2xz7svkqKendG4tdYEdqzR0rp34GFt7XzNgtY8WWGduXa2dtzBC9kRu5xBBI0I4jtUePajbva4NLTbrhtnE9o+a+KLY8znEkBuBu9IdcYmg54cs1avc0adbMg5aW1+9dfe3L3rnMZJpX0u1GMaMrPH1v5dy2S7WZI/eOe5sjW2bZuuv5lT4ZIyQJMieItbsvyKntp4xbLUjUNUx8eOXO0yysUrNrx4CS92Oxw5G+fapGyNuRRuaZpTkcRsw/LIdqtfR4yPZHyb9io9vxsEbQ054s72HArd8frjeUllvS7rOlNG49UuIIIJLXC32Zr76C1rZKqRrRpF9l2rhjF2rrP0bMIrJSRYGH/c1efTp09HREVBERAREQERR6reWbugCb9a54ch2oIe2Ng09YPpW2eNJG5OH5jsK4LbXRaopLutvYv32jT+IcO/Rd/HJVWuWtyBFjliN7c+Wd/sWQ+q1wC5sC0kWHMixv8APkg8lYbaLayVd3tLoi2pL3tDYJOBaOq/W923NuGYtqdVxm09lTUr8MzC2+jtWu7j/RQfDXcltD1CDiFtbKs2KltcpdLVlhUKmifK8Mja57joALldbsnoho6qd/8AW0/6j+XzWfVdq/a1BJtGKnfTtxSMcY3Z2s0i4JPIW+1WmxehMENn1BE8gzt9QfD63x+S6aGFkbQ1jQ1o0AFlsXRlgCwsMgsoiAiIgIiICIiAuF6YPxVljo1jQPjc/ku5XA9M2ltbcD2ms/EfgrHTx/6vs+HVNJhG8wFwDbYWjPL62XbnnqqyrlAmeYQ1rb2aW8R39qg2zuVPoaNzhjIbgH330P8AXBVzao6maNrXXc3HniJIv/JSKasMby8Oe0tAwG1wT28uKuX9FRNEx7JLEi+F2YF+Srp9jVUH/TxNHFnWHy1QfdJA6qle8vAJDpCQL58cr5aqvrqt+CNrg0jENL8B/NDVzPBiZhjsSSWizjfUE8Qoro3l1pHWDdLZnPkuH4Vvk9reHa+X2w9b+z7dXYY5mWPWBtyva111tdtOA0IguS8xNyGdiACLn4LiHRkvsMyTpquso9jCnopZph9Lu3Bo/dxC3zzXSYyW1nD+rHTtKGTHDE86uY0/MLeo9BGWQRNOrWNHyAUhVjLu6EREQREQFUdINkOq2xBpbZjiSHXzytwVuiDk6Tocf+vIC0fVZcX7yc/krCu2EXug3e7ayIYcJB0ysB8leIl54Xak/VMwlc9pisdBnl8VErej00kTWNfE0gEE2Jvf7l0yLHpF9q5p/R2UiPrsuyMM1Nr5m+nd9qgwdE6prml07CQCDm4jM3va2vDVdmi38aTbj63oaZpQ5+7LQAPacLa34ZrVJ0Hw5w7sPGhLnCx7gM12qIjjR0YrbD6aD9m6MizrEG2nLQLaOi0+ZLoC43scLsvzXWog409D5SMzTtdzDXEH4XFlsZ0XqW2GOnLeWFwt3ZnLsXXIkuruF5cjL0YqXZB8A7QHXUSXoXVOeHCaH4gn7wu5RaudvaSSOHquhU72tDZIALNxDCcyNTe2V+Stuj3R+SklL3uY68eHq3uTcHlyFvguiRZUREQEREBERAWuSFr/AGmh1uYutiIMNaAAALAaBFlEGF8TwMlYWSND2nUOFwVsRBxW2ehOr6Q//U4/6XfgfmoWyOhU0pDqg7ln7urz+Dft7l6EsIIuz9mw0zMELA0cTxPedSpSyiAiIgIiICIiAiIgIiIC5fpxQGSJkw1YcLu46H4H711C1zQtkY5jxdrgQR2FG8MvW7ecUvR2R8zY8bLEFznM6wYOFzkLnkrWbZscELmtzLSOtxKkxRTUkno+87YS/wBmQfu3+q5aNqPle14Me7Fxiu4H5WWjPH1qRQVtXJTtMMcXUOHruN3W1sNB81th6RMDxHUsdBLe1jm35qXstoFPFYAdUJtKgbUx7uTS4OVicu9Rh8yGnmdfAyQ2PWw3+1Vtb0diIx3c3U4Rayu4YWRizGtaOQFlrrPYKDn6ahii2iwNblhBzzzsVb7RG+lhpRnjcHydjGm/2nJVNZUiKuD7F1mizRqSQQAPiuk2LQOjDpps55bF390cGjuR1w/L+e/t9/8AxZrKIo5CIiAiLla7ZFU9lUGEhksj3luLMlvsBvIOOG9/3e1B1SLnZX7QMsgaHtYXAA/RmwErBdvfHjOd/msyO2g10bWh7g2Q3ccHWZvLdb/6+Vvmg6FFydBVVFSS7FjLZGuDLNOBpEtnXt1SeqMNyRbXNZ2hWVtPG0OkN37vruwNscDzIAbaAtabWN8xxuA6tFS7RdV4YPRnOc0s6zg1ty7q4S4Ot1faJtZaZBtAB5aSSd9ZpDLNAeN3h7SzFa/ZeyDoEXOSzVscD5Xve0RwyuF2suXXODF8M8rcLrXDWVLmQ4JcTpnyRO9k7s64gcIDsLWuF7WJIQdOiqdoPqhLhia5zHbmzhhs2z/pb3zzZ3qE9+0N28BspdvMnfRg4bO0GYtfCNePeg6NFyY2jWPmlja/DK2OzY+rnJu2OItbKxcTcmxuBwur7ZBm3R3+LFiOHEAHYeF8Jtz+FkE5ERAREQEREBERAREQEREBERARFhBlRa2vjgDceIl5Ia1rS4kgFxsB2AlSlC2pQGoYG4g2xv1mBwORHeCL3BBCD7nr4mMc9zxZsZkI+thAvfDqs0lbHMZBGb7t2F3fYH8VUSdF2OLyZXuxMIxOuXYjFucV72PVz01OvBT49kRgPDsRxPL+q5zNQB9Ui+nFBIkromyCMvGI3yvpYXz5L6ZVxuLgHDLjcZiwNxzGYVRUdGhIX3l6rt7bqC/0ubru1NuH4rdV7AZJis8sDnh1mtHs4AxzO4gILBlZGfrgdYtFyBcg2y5rY2dhcWB7S4atBFx3hVT+jzCXkvzcH26o6uN+PLusvjYuzpYpqh0rQA8us4HOznvdYWOnWvmAb3QW3pUeEu3jMLTYnELA8ieBWX1Mbb4pGC2t3AW01+Y+aqB0e+ibHvRZpaAREBdrQWjERmT1r3uNNNQdX6khjhdGZmYzu+u4C/Ua1o0IP1b5EaoOgBWVDpaiJkbGb5ji1obfE0XsLXsMvkpYIOYNwptNsoiKqIiICIiCPW0cc8ZjlbiafmDzB4Fc1tHZdTExzReojNrOHti3MfW+C61YRvHPXF5jm9lbRh3TIzI1r2ixa7qkfNWO8HAg/FTKiiil/aRsf/E0FQj0do/cAdxcPxVX/Lv1n9/+mqasjYLvkY3vcFCdWuqAWUsTpL/XcMLB8Tr8FcQbFpYzdsEYPMi/3qcAhvCdTf3/AJ/yqtm7FET99M7ezn61sm9jRw71arKKM5ZXK7oiIjKl6T7cNBFHI2MSY34bF2G2RPI8lTUnTl0geXU4GEXFn3ub2zy/NbP0jxudTQYWkne8Bf6rlxuzm5PDnFlg3Kx62eQ/9qJvl2tB0xdKAXU9gb+y+5+0BfNV05bH/wDrP1tcuFvsXN7PazdsxybsXdZ1jrlyU6d8b45Gul3rmx3vgPLK5/NZ/N8crVvD04jfII93gcbWxutr22Uqp6SyMeAIA5v72I3+VlQQU0RaGte19wOGnYVEp9qlsz4pQN37LTY3bwt/Wi14fJM8rhrr5ZzlxksdTF0kJLmiEYrYg3rDF3ZKNSdMN/EXinva+Rdra1+Haq00bQRM6aRov1HADIjnfjktNNBTxBsbZJHOa4kXa3EcjcDPMEXWusuelx3cXS/8QSmYRtgBLs2EuIBABJvlwUtu0piR9BbncnTU2yzKoNl1Qklic03GOSMHsc0kfaF0sjiATlkCf/H+al4qzqKmu6SyxucI6UyRj6xfhv8AAi6jP6WStexjaRlzl+1GXPQKtqqprnPmLnOY05Dm7gAPtUOnpN3E4PeRJJcucct2wnP4n8+SmV1j+tdfHjMrz1HSHpVJcgUw6v7Q7wWb2DmVqZ0xe512014rkYi+xNuQtmqJtI2ZzGAERC+7ZmC+2rnDl3/mt9LPfFgbieC4Yi2zGBpLRnodFLl6Y7vNMpjbx0uIulpwvfJAGkG1gSXHW3AZZHXksDpk50O9FMWjgHusT9i5+CTdiSd/0ha5wI4HXPt1K+658hiJlycbWGWQz4DRcsMsrz8Hkxkv0X0/TPCQGQF1+ZIz7MtO02X1RdKZpXtb6M0AuAJxnK5tkMNyqano42RtlMgYTewd1uJFw0Z/cplNXgvjbGHykvF3ObgGudmixPxK7SyTXblbvqadsiIqCIiAiIgIiICIiAiIg1zyiNjnuya1pce4C5VFsPbDjHC6qf8ASVj3OgjDfZYBkDbhYXv2qz25C6SiqWN9p0TwO/CVz2zXF8uyqhjS+M07oSWi+7cBqeXskIOl2jtCKmj3kxwsxNbe2hJtnyHaq/Y9bL6VVUtQ7E+Nwkjda1436C3ZouWo6KVuztoxgyTvkqdy0a2IcBjPK9xfuCv6FuLbMpBuIaRkTz/eLsQ+wIOkREQEREBV21NsR04Nzd3Ll3/kte3trCmiNvbIy7P5rz/eS1M2tydTwYPxK55ZXqM73xFxU7dqKl+BhLQdTbIDnb81iSWUD6MNv/fHD81Z0tC2JuFvxN8z3qDXWbLgaAMrnMm6uOOuz1jU2ondbebvIaNZb5m+alUtS8E4SWkX0Nsu1aqandJfPhfM2XzBVGIlpja69xcutbO3IrXrKlkdBQ7bvYSjL98fiFdNcCLg3B0K4KlL3g2ABbrnzV3siudEQyT2HaZ6X/BT+n7G9dujRYWVpsREQEREBERAREQEREBERByf6QpXsp4N24tJltl/C5c1TyPkhhx5kuNzYXyIA+wBdh0x/Yxm7BZ/19NDp2qkoYomwgGQFzb2wWIOZ+WijO+VAcQgAZbFiNr6cFiOVxjIsWutmQLAi2l+KlMaBG7K9sWdr2zUiCCF0Ra8PDg29mgW04pZ+Sfdqd18Uch37sgPZvbjlrZaKd7ZZJiRc5nTT4K0fRxskG7Ml3e0HjQ5Wso2zKC7pGSiQhtjZuoOf8lPDxnll8rZLJKhuqXNlwXJaNGtAyJW+GcNILHA5G9z3WtYcNQrCSiphIRabeFoPWAta41K1inYcsBc1rSBhF7Z9yznbbN8l4t00UbxGA1rrk3diAtnfHfT+IfEKxpa6V9PKXPe4EBgzGpuOAVa2JrRcMwkZA3BV1V1DN1ELNZfrgsHLIHTXVW5bSKvdC4vmyO+ROTn8z2DILVKXGxcMi6+E6l3MjiRwboFvc4OIDWl1smi+fMZfH7VqmiIiaREWXIAub2zta+vFN34dLeNPiaW7mCTrWxAsaczfmRztopFA28Ra84I8cvVv/eOWfepIomtdEW9frYiGtvY2Oq3bPm3bnARguxyG7hh43zv3pMdY1j2tqmhhcIZ3NJAG8Abaw09ok8Fv2yTh69gA1os3Xjrmc1IqjLJHVWawEtkBANuH2lfdXSyFtpcFyy4wjhbj2rOE1j+7p5LvLaPQQF+MgABrrZ3N8geFuasYYnsey5iAxt+qR9uJaKGkilYXMM4xE3aH2zGWg7lup9g00ckbnB2PE0guzzByzIuukk+axa7JERVkREQEREBERAREQFCq9r00DsEtREx37rngH5KW5wAJOgFyvMqKOhdTTV+0WPe6adwjDXG548CO3XkEHd/8R0P9rh8YUPZu0dm0okbFVxBr3l+EyCzSdQ0cAoOzOjOyauBs8MLix19ZHggjUEXVV0L6N0lXTyvnjLnNmcwHG4ZANPA9pQdD0Nma9lWWODgauUgg3yNrH4q12dsyOm3mC5Mr3Pe5xuST28houQ2lQO2LUNq6UONI+zZo7k2+J+w8DlxXa0dXHPEyWJ2JjxcH+uKDeiIgLDjYEnQLKibUfhp5D2W+eSluoluo4PbjpaypIBswa5/1oPxUyioWRNAGQGfaTzKRNGZAyJv396+Kme3VGbjy4BTCa7TXC4dtmmGO87Ba/HMZcFX1EjJZy5jg4WbYjRU7oGkkWADtdM+Cl0kQYQA0kHM24hdIl3YtKhj4I8TJoxoHWdiJKpTiJAceZ0txKlSUlNCMUMN3A5AFxsOORK+3NL3MIaL6AOHNNJuRrx7oOIaS6xFhqVri2i9zxjY+2E6Z/NXMzI90cbZd5a+UZcL20uNRpmqtsTm2eDYnP8APsKi39XYbGqt7A0k3Lcj+H2Keud6LSkmQHlf5H+a6JYx6XHoREWmhERAREQEREBERAREQcr0/beng/xf9rlQ7FaME1x7LQR33suh6dNJggsCfpf9pXObNacMgsLEAm55G+XNYvblb+Z90Ngx/c4WtcHrKc+mxF4DjcxjO5GZGnctNPHeFrQM8Tv6+xS3VbA5uEcrEAWyGef9FXLnGa+rcy1ld/R8zl7p7Pdctw2PLILVsskTS3fexyzyyPAL7opzO4ycSc+Gihwy9dxaD1nWBH33WPBz5M41llxjpYvJ9KbY2vHf7b5fLRfJcd4MXWuCMxfQ6/YomN2/JzOG+buA7O691sppnzSMAsHXwgk3vcE52GWi1ZdxLfzWJU8rCcLwXG9gALn4WWyagcyxc3E1rbDIHtUzZ+yhG/eyEOlzAI0A7Fa4Rl2hXTTlmztErmAANAGg7L527lukLIy3EwC5uSAMwMypNZshoeXtvckAgHgcj9/2L6qaXBgdicTjYM7c7clMe+Wsta4DM1skYubOzPwB05FaaZrXiVztQ+UDLtFvwWqGKMyQgtBc7eOcOYBdYnmqsbQdTxutfOaQHictO7JbymsMmPHjc89ROgmBp6wkjFaXDe3AZKXtNouwt03Sp9myBzDcBwdIQQc7tIuQR8ArarcDYgW6tlw8N/Lfu7+eazsStmxARNIIb1nfirCINcRYtJBF8u1chPUMAAZI5xBdiBJsAbfDVTaauiL47P612DMHmF6MvHlJuTbze0dqiIo0IiICIiAiIgLCysIOa6WdJYKeKanxk1DoyAGi+EuFhc8Oa491bQS7Mp6V8skcsRLsQjxC7iSRqL6/Yq/pgf8A8nVfx/gF8M6M1zmbwUsmG19M/lqg67YXSjZ1FStga+V9rkuMdrk65XyULof0ppaOnljmL8TpnPGFt8iGj8CuJe0tJBBBBsQdQVhB6nP052bIxzHiRzHAhwMeRB+K5ro90mjoKmSNr3yUTyS246zDwy+w89VyK3UlHLO8RwsdI88Gi6D1D/j+g5y+D+audj7agrWF8D7hps4EWI7wvIa/YVXTNxzQPY3961x8SNF1P6Lj9LU/wM+8oPRVD2s29PJ3X+RCmL4kYHNLToRZSzcSzccJS04kDrucS05gC/dovuWBjM35H+uF1qrampppXMYY2tvxYcz2nELqqpKl09S50rsTuQuARoMuS1jeGPjayFOH5tBAH1uCkGVkcgLsYAHAX0W6KYNaXOIsBZx79B2rmdrVz5Jg2MEm3Am6tMbv7Ogr9pRPbZhlBPNn9ZrTUlwYzC4iwGZ1zJN/t0XLvq5I5RjBBBBsXFdDLO6VrcIycL3OgAuLfMIWa6XDdrzkf9AngQ0i3wuquWSQXDnB1yTkLDPUDsUinmONsTrBztM7fYplbFhjBFiTlm7+rpTlM6KMzeez7z/JdGqzYFPggxHV+fw4K0XPHprHoREWmhERAREQEREBERAREQct09J3EFiR9LwP91y5ajJD28jcHLnp+K6jp7+xp/8AF/2uXNUz92cRc0CxFiL3v/6WL25Zf1MbVY800QiBLt67TjqFp2PUzNLGvya72ibW7zfuSXaETY2bxmIlzjcDTP8AmvuofTG+ABzjkCWWt8bqxrPXyvdnVoeZHy4GOyvo34qn9MYSGNb1gc8OgbxJsc7j71HMTTYuwucONszy+Smbao44I6dzWhry4hxGV+qTnzzWcPH6Z5Z77JnLqfRFjqHMrJXNa1zL3AJ6rri1iVd9GaMh7pMLQMVxbgbWsPmfmsUGwMUUb8TWl4Djlnmuiggaxoa0WA0Vntb+j05+mtztvLlsedO5aV9yHRbcSTPNRK9jnMGAXIcDbuKlsNxZfByQc9VOmiML8A6lwcIBPWHK97KgnlLmmNzC1zpHHMZWJ7suHFdptDZzJmjg7n+aqHdG3H67B8CufMlx7dvH6y+29VS7PlG8iu7AMRzP8PJXpmD2Ag31BystP/Cx4yM14sJ/FbJKH0aMMu0536rbahSb1rTXluOVuUu7VTs+VjjFG7J2/cL4RmCSdT+SvXbLLCHBzCA5p60YB9oaOFrfJVmxdmPmjDw5oGN+Zbf6x7QrZ1DNHa8wLQ5twGnmO1d/JfbPVny4THGY726tERZQREQEREBERAWufFgdh9rCcPfbJbFhB5hsaOSI1e0q6NzpIgMAkbhxSOyBtbhl81ujpNszw+ntqHAEF7YxIQS3sZbDbsOq7XpNs11XRTQs9sgFvaQbgfGy4ql6Yz09J6E6md6Qxu7aTlbgLtte4+1BH2xGdpUUVdHETUtfupxG2+KwuHWHZb59i579U1P9nm/y3fkvUuhGyX0lEBKC2SRxeWn6twAB8h9q6FB4Z+qqn+zzf5bvyXVPM1DBS0NGMNZVND5X6OF9G34Wz7rdq9JXFdNKWaCrp9pQMx7oYXjla+vYQSL8EFbI/aGypIzWSioppjheC8vFjr7QuDbPkVu2Vsyoo9tGOna/0V5xFwb1cFsQBdbgTZRto7Vl25LBTQwFkbXYnuJvbgSToABfvXpDW2AA0AsgysrCyg5vpdsT0iIvZk9ud+7j+BXnVJTytlOJpDmnO+d/zC9oVFtbYAku+IAH938vyWeYzZpycjyY2Mvc5ud3nID4AfavuNsVs2gd4v8AbqvuaikjJBBB7vwWmxGRK1Mt9HFhUU8Lh7APLK1vxUpmFrIWi1wCf/I5KK6QCy+d0556rSrvXNS4yTTZV1EcczC4XyJyyIy4371cbGpPSSHBpDNSTy+QzK17P6LmZzHzDJuYv+R1+OS6+ngbG0NYLALNvt9kk3NNjWgAAZAaLKIq6CIiAiIgIiICIiAiIgIiIOS/SH/8en/xv9rlylPTbwAuLrA37113T4jc04PGb/a5UkfVFhkrMZby5Z52dK/aVJGyFpdiLQ++RF8/gq2GOG4wukB4XII+5WHSCYbpsZNsRufgqKIYXgtBKzdS8Ovjlyw3U57yw+12/krSSv8ATHxh0V7eyL8TqVVRQZ3I1XXbB2Zu2iRw67tOwLFtyuo9E8ePjx9r2t9n0zYWW+sc3Ht/JTQ5aGhbAtyacLd3dbLr7cVqC+iVUZBRy+boCgyCsFCvm6CjrayqhdZzm2PsuDRY/kexaZKp8kIdIbnHbS3BX00TZGlrgCDqCuf2hE2CINDsQMlxfuOX2LlcbLvbt743HWuWrZVXLBTswluFz3kZXPtFT/TpZC1pkZYubfLtHYqV8mCjiIP13fe4rRTVOKaIYv8AqM5/vBYx8mWWVv610xwlwepIiL0PKIiICIiDVv2c09IZzUFRamvZFIGPvmxz7gXyBa21hnq8ILj0hnNPSGc1R/ren4yAC17kG1sOPW1r4bm3Ysv2nGI2yA3YXhhOmEnLMHTOw+IQXfpDOa+TLHe+V+dlz8G3YXNxODmCzSS4aF1+qbaEWz71v/WsFyMZuDa2F2ZuW5ZZ5tOnJBd+kM5p6Qzmqf0+O8IGJwm9ghptpfM8FEG3ot8+N3VwFwJxA+yQM2g4hckWyzQdH6Qzmm/ZzVEdsQC5LurYEEAm+RJyAuLYTe6wzbERc5pu2zi0Eg52tc6ZNBcBc5ZoL1ssY0sO4LPpDOapW7ThP1jxN8LrAC9yTbIZHM5ZL4/XFPYHeam3suy01FsvabrzCC99IZzT0hnNUZ2xTgC8gHVLswdACTfLI2a7LXIr6h2nE+VsTcRc4OPskAYSAQb6HrD+iEF16QzmnpDOapP1pEImyvdga69r56XvpwyvdfX6zhxFuPrAhpGE6m9rZZ6HPsQW0ronizwHDtChSbMpXcLdxP4qCdswWxB/V1JIIsMJdfMZizTpyX3+tYcusczb2HZG+HPLLMgZqWSpZL2kN2RSg8T/AF2BS4IaeP2WtB52ufmVTy7YY2q9HLTiuBe44txXw3vbLVbGbWgdYNfdxvYYTfKxN8stRrzCesPWLz0hnNPSGc1R/reHDiLiB1b9UkXdawuBYnMZdqmg3F1VT/SGc09IZzUFEE70hnNPSGc1BRBO9IZzT0hnNQUQTt+zmtclfC295BkC63Gzdctcrj5hVG0HnJoIAtiOI4QRdosT/wB33KDU07Klu5jc6IgjEIwG4TchxvxyGg4WQdT6Qzn9iekM5qpoiDE14BGMB5uScyAeKkIJ3pDOaekM5qCiCd6QzmnpDOagog+9o0dNVNa2duMMdibm4WPPKy0/qqj/AHB4nfmvtESyVFk6O7Oe7E6EEnm5/wCay3o/s8aQjxP/ADUlFNRuZWdVobsOgBBEQuM/ad+anCKDl960IkkiXK3tIwQ/1dMMP9XUdFUScMP9XT6L+rqMiCTaH+rpaH+rqMiCTaH+rrGGH+rqOiCRgh/q60VNBSSi0jA4A31KwiDYylp26NHLUr6EFPe9hf4rSikknQnekM5p6QzmoKKid6QzmnpDOagognekM5p6QzmoKIM4DyUSbZMcjnOexxcbZlzsrEHq59XNoOXJWqIKj9TQ4cO6u3kST9Us/wBJIWx+y43QmBzC6M6gkm+d9dVZogqJNiwuxXi9pznOsSLl1gdDxsPkk+yGub1QWOvcOFyRdxcbZ83H5q3RBApaXdRMjbchjQ0X42yzWmTZETr4o74r3zOdyHfeARy4K1RBVfqmL9w+zh1OliOfJxR+yYnEExnIk6kXvYkHPMdUZHLJWqIKs7LjNuocgW2xGxBvk4X6wzOvNYZsmJoADDlpdxPFrufNrfkrVEFQ7YsJveM9Zpa7rHMG975/3itzdnsEm8DSH9bO5+thvcf9rfkrFEFR+pYrAYH2BxDruyJve2eQN9AvqHZETH42xkOvi1Ot3HTve75q1RBUP2LCW4TGbYMGp9kBzba8nu+a+anYzZHtf1m9bE4D6xuHZ56XAVyiCrl2VG95e5jiTYkYnYSQLAlt7Xt2LW3YkIsN2bB2LNztbAAnPP2R8lcIgqf1PFhc0NeGuABAe4DKw0vrYDPipbIcLQ1rbAAADsClogi4DyTAeRUpEEXAeRTAeRUpEEXAeRTAeRUpEFHtOhnkdHu34cL7izOAFy1xJ4kDhy1Wx1DLJhDmloZmHbwudmM+AAOZF7nJXCIIrYiAAG2AyAWd2eSkogjbs8k3Z5KSiCNuzyTdnkpKII27PJN2eSkogjbs8k3Z5KSiCNuzyTdnkpKII27PJN2eSkogjbs8k3Z5KSiCNuzyTdnkpKII27PJN2eSkogjbs8k3Z5KSiCNuzyTdnkpKII27PJN2eSkogjbs8k3Z5KSiCNuzyTdnkpKICLzT1hVnu6fwv8AMnrCrPd0/hf5kHpaLzT1hVnu6fwv8yesKs93T+F/mQelovNPWFWe7p/C/wAyesKs93T+F/mQelovNPWFWe7p/C/zJ6wqz3dP4X+ZB6Wi809YVZ7un8L/ADJ6wqz3dP4X+ZB6Wi809YVZ7un8L/MnrCrPd0/hf5kHpaLzT1hVnu6fwv8AMnrCrPd0/hf5kHpaLzT1hVnu6fwv8yesKs93T+F/mQelovNPWFWe7p/C/wAyesKs93T+F/mQelovNPWFWe7p/C/zJ6wqz3dP4X+ZB6Wi809YVZ7un8L/ADJ6wqz3dP4X+ZB6Wi809YVZ7un8L/MnrCrPd0/hf5kHpaLzT1hVnu6fwv8AMnrCrPd0/hf5kHpaLzT1hVnu6fwv8yesKs93T+F/mQelovNPWFWe7p/C/wAyesKs93T+F/mQelovNPWFWe7p/C/zJ6wqz3dP4X+ZB6Wi809YVZ7un8L/ADJ6wqz3dP4X+ZB6Wi809YVZ7un8L/MnrCrPd0/hf5kHpaLzT1hVnu6fwv8AMnrCrPd0/hf5kHpaLzT1hVnu6fwv8yesKs93T+F/mQelovNPWFWe7p/C/wAyesKs93T+F/mQelovNPWFWe7p/C/zJ6wqz3dP4X+ZB6Wi809YVZ7un8L/ADJ6wqz3dP4X+ZB6Wi809YVZ7un8L/MnrCrPd0/hf5kHpaLzT1hVnu6fwv8AMnrCrPd0/hf5kHpaLzT1hVnu6fwv8yesKs93T+F/mQelovNPWFWe7p/C/wAyesKs93T+F/mQelovNPWFWe7p/C/zJ6wqz3dP4X+ZB6Wi809YVZ7un8L/ADJ6wqz3dP4X+ZB6Wi809YVZ7un8L/MnrCrPd0/hf5kHJoiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIg//Z\n"}}]}}, "d45f29ea07a2450c94a81bac3ebe2b78": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "45e9194a1d654b39ba032665cb59ccb1": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_abe4d5bd81a8486e9df18be87b47ee64", "IPY_MODEL_697c5b1cf70e48088f9f0c1d6ebf64bb"], "layout": "IPY_MODEL_d45f29ea07a2450c94a81bac3ebe2b78", "selected_index": 0}}, "b2c6899e1c0a4f639c047841d4d6d83c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "66ec7fd1d0f04c81a482b95b9d44c6b8": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_b2c6899e1c0a4f639c047841d4d6d83c", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://www.bilibili.com/video/BV1WM4y1T7G5\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<__main__.BiliVideo at 0x7f912812c290>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1WM4y1T7G5&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "}}]}}, "376fcb3ee33f47249936a3a6fab81f46": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d9113593f44145b4b9f08121be2fba8c": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_376fcb3ee33f47249936a3a6fab81f46", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Video available at https://youtube.com/watch?v=dKaOpgor5Ek\n"}, {"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f9128207e90>", "text/html": "\n        <iframe\n            width=\"730\"\n            height=\"410\"\n            src=\"https://www.youtube.com/embed/dKaOpgor5Ek?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ", "image/jpeg": "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\n"}}]}}, "48a5fb5683c74d37b93f628123327ae1": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "08a93ff8938446cb9627dbd000fdb7bc": {"model_name": "TabModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TabModel", "_titles": {"0": "Youtube", "1": "Bilibili"}, "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TabView", "box_style": "", "children": ["IPY_MODEL_d9113593f44145b4b9f08121be2fba8c", "IPY_MODEL_66ec7fd1d0f04c81a482b95b9d44c6b8"], "layout": "IPY_MODEL_48a5fb5683c74d37b93f628123327ae1", "selected_index": 0}}}, "version_major": 2, "version_minor": 0}
</script>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W3D4_BasicReinforcementLearning/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W3D4_Tutorial4.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Tutorial 4: Model-Based Reinforcement Learning</p>
</div>
</a>
<a class="right-next" href="W3D4_BonusLecture.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Bonus Lecture: Chealsea Finn</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By Neuromatch<br>
  
      © Copyright 2021.<br>
</br></br></p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>