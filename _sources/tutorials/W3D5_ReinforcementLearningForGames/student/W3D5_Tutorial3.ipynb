{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D5_ReinforcementLearningForGames/student/W3D5_Tutorial3.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a> Â  <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGames/student/W3D5_Tutorial3.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 3: Policy-based Player\n",
    "\n",
    "**Week 3, Day 5: Reinforcement Learning for Games**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Mandana Samiei, Raymond Chua, Tim Lilicrap, Blake Richards\n",
    "\n",
    "__Content reviewers:__ Arush Tagade, Lily Cheng, Melvin Selim Atay, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Melvin Selim Atay, Spiros Chavlis, Gunnar Blohm\n",
    "\n",
    "__Production editors:__ Namrata Bafna, Gagana B, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "In this tutorial, you will learn how to implement a game loop and improve the performance of a random player. \n",
    "\n",
    "The specific objectives for this tutorial:\n",
    "*   Understand the format of two-players games\n",
    "*   Learn about value network and policy network\n",
    "\n",
    "In the Bonus sections you will learn about Monte Carlo Tree Search (MCTS) and compare its performance to policy-based and value-based players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tutorial slides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/r6tup/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These are the slides for the videos in the tutorial. If you want to locally download the slides, click [here](https://osf.io/r6tup/download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install coloredlogs --quiet\n",
    "\n",
    "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
    "from evaltools.airtable import AirtableForm\n",
    "\n",
    "# generate airtable form\n",
    "atform = AirtableForm('appn7VdPRseSoMXEG','W3D5_T3','https://portal.neuromatchacademy.org/api/redirect/to/95651a47-083f-406e-a6cd-8af24670c5bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import coloredlogs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pickle import Unpickler\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set random seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Executing `set_seed(seed=seed)` you are setting the seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set device (GPU or CPU). Execute `set_device()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download the modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Download the modules\n",
    "\n",
    "# @markdown Run this cell!\n",
    "\n",
    "# @markdown Download from OSF. The original repo is https://github.com/raymondchua/nma_rl_games.git\n",
    "\n",
    "import os, io, sys, shutil, zipfile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# download from github repo directly\n",
    "#!git clone git://github.com/raymondchua/nma_rl_games.git --quiet\n",
    "REPO_PATH = 'nma_rl_games'\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "  download_string = \"Redownloading\"\n",
    "  shutil.rmtree(REPO_PATH)\n",
    "else:\n",
    "  download_string = \"Downloading\"\n",
    "\n",
    "zipurl = 'https://osf.io/kf4p9/download'\n",
    "print(f\"{download_string} and unzipping the file... Please wait.\")\n",
    "with urlopen(zipurl) as zipresp:\n",
    "  with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n",
    "    zfile.extractall()\n",
    "print(\"Download completed.\")\n",
    "\n",
    "print(f\"Add the {REPO_PATH} in the path and import the modules.\")\n",
    "# add the repo in the path\n",
    "sys.path.append('nma_rl_games/alpha-zero')\n",
    "\n",
    "# @markdown Import modules designed for use in this notebook\n",
    "import Arena\n",
    "\n",
    "from utils import *\n",
    "from Game import Game\n",
    "from NeuralNet import NeuralNet\n",
    "\n",
    "from othello.OthelloLogic import Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helper functions from previous tutorials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper functions from previous tutorials\n",
    "class OthelloGame(Game):\n",
    "  \"\"\"\n",
    "  Instantiate Othello Game\n",
    "  \"\"\"\n",
    "  square_content = {\n",
    "      -1: \"X\",\n",
    "      +0: \"-\",\n",
    "      +1: \"O\"\n",
    "      }\n",
    "\n",
    "  @staticmethod\n",
    "  def getSquarePiece(piece):\n",
    "    return OthelloGame.square_content[piece]\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "  def getInitBoard(self):\n",
    "    # Return initial board (numpy board)\n",
    "    b = Board(self.n)\n",
    "    return np.array(b.pieces)\n",
    "\n",
    "  def getBoardSize(self):\n",
    "    # (a,b) tuple\n",
    "    return (self.n, self.n)\n",
    "\n",
    "  def getActionSize(self):\n",
    "    # Return number of actions, n is the board size and +1 is for no-op action\n",
    "    return self.n*self.n + 1\n",
    "\n",
    "  def getCanonicalForm(self, board, player):\n",
    "    # Return state if player==1, else return -state if player==-1\n",
    "    return player*board\n",
    "\n",
    "  def stringRepresentation(self, board):\n",
    "    return board.tobytes()\n",
    "\n",
    "  def stringRepresentationReadable(self, board):\n",
    "    board_s = \"\".join(self.square_content[square] for row in board for square in row)\n",
    "    return board_s\n",
    "\n",
    "  def getScore(self, board, player):\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    return b.countDiff(player)\n",
    "\n",
    "  @staticmethod\n",
    "  def display(board):\n",
    "    n = board.shape[0]\n",
    "    print(\"   \", end=\"\")\n",
    "    for y in range(n):\n",
    "      print(y, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for y in range(n):\n",
    "      print(y, \"|\", end=\"\")    # Print the row\n",
    "      for x in range(n):\n",
    "        piece = board[y][x]    # Get the piece to print\n",
    "        print(OthelloGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "  @staticmethod\n",
    "  def displayValidMoves(moves):\n",
    "      # Display possible moves\n",
    "      A=np.reshape(moves[0:-1], board.shape)\n",
    "      n = board.shape[0]\n",
    "      print(\"  \")\n",
    "      print(\"possible moves\")\n",
    "      print(\"   \", end=\"\")\n",
    "      for y in range(n):\n",
    "        print(y, end=\" \")\n",
    "      print(\"\")\n",
    "      print(\"-----------------------\")\n",
    "      for y in range(n):\n",
    "        print(y, \"|\", end=\"\")    # Print the row\n",
    "        for x in range(n):\n",
    "          piece = A[y][x]    # Get the piece to print\n",
    "          print(OthelloGame.square_content[piece], end=\" \")\n",
    "        print(\"|\")\n",
    "      print(\"-----------------------\")\n",
    "\n",
    "  def getNextState(self, board, player, action):\n",
    "    \"\"\"\n",
    "    Helper function to make valid move\n",
    "    If player takes action on board, return next (board,player)\n",
    "    and action must be a valid move\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      player: Integer\n",
    "        ID of current player\n",
    "      action: np.ndarray\n",
    "        Space of actions\n",
    "\n",
    "    Returns:\n",
    "      (board,player) tuple signifying next state\n",
    "    \"\"\"\n",
    "    if action == self.n*self.n:\n",
    "      return (board, -player)\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    move = (int(action/self.n), action%self.n)\n",
    "    b.execute_move(move, player)\n",
    "    return (b.pieces, -player)\n",
    "\n",
    "  def getValidMoves(self, board, player):\n",
    "    \"\"\"\n",
    "    Helper function to make valid move\n",
    "    If player takes action on board, return next (board,player)\n",
    "    and action must be a valid move\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      player: Integer\n",
    "        ID of current player\n",
    "      action: np.ndarray\n",
    "        Space of action\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray\n",
    "        Returns a fixed size binary vector\n",
    "    \"\"\"\n",
    "    valids = [0]*self.getActionSize()\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    legalMoves =  b.get_legal_moves(player)\n",
    "    if len(legalMoves)==0:\n",
    "      valids[-1]=1\n",
    "      return np.array(valids)\n",
    "    for x, y in legalMoves:\n",
    "      valids[self.n*x+y]=1\n",
    "    return np.array(valids)\n",
    "\n",
    "  def getGameEnded(self, board, player):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      player: Integer\n",
    "        ID of current player\n",
    "\n",
    "    Returns:\n",
    "      0 if not ended, 1 if player 1 won, -1 if player 1 lost\n",
    "    \"\"\"\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    if b.has_legal_moves(player):\n",
    "      return 0\n",
    "    if b.has_legal_moves(-player):\n",
    "      return 0\n",
    "    if b.countDiff(player) > 0:\n",
    "      return 1\n",
    "    return -1\n",
    "\n",
    "  def getSymmetries(self, board, pi):\n",
    "    \"\"\"\n",
    "    Get mirror/rotational configurations of board\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      pi: np.ndarray\n",
    "        Dimension of board\n",
    "\n",
    "    Returns:\n",
    "      l: list\n",
    "        90 degree of board, 90 degree of pi_board\n",
    "    \"\"\"\n",
    "    assert(len(pi) == self.n**2+1)  # 1 for pass\n",
    "    pi_board = np.reshape(pi[:-1], (self.n, self.n))\n",
    "    l = []\n",
    "\n",
    "    for i in range(1, 5):\n",
    "      for j in [True, False]:\n",
    "        newB = np.rot90(board, i)\n",
    "        newPi = np.rot90(pi_board, i)\n",
    "        if j:\n",
    "          newB = np.fliplr(newB)\n",
    "          newPi = np.fliplr(newPi)\n",
    "        l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n",
    "    return l\n",
    "\n",
    "class RandomPlayer():\n",
    "  \"\"\"\n",
    "  Simulates Random Player\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game):\n",
    "    self.game = game\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates game play\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      a: int\n",
    "        Randomly chosen move\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the valid moves using getValidMoves()\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "\n",
    "    # Compute the probability of each move being played (random player means this should\n",
    "    # be uniform for valid moves, 0 for others)\n",
    "    prob = valids/valids.sum()\n",
    "\n",
    "    # Pick an action based on the probabilities (hint: np.choice is useful)\n",
    "    a = np.random.choice(self.game.getActionSize(), p=prob)\n",
    "\n",
    "    return a\n",
    "\n",
    "class OthelloNNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Instantiate Othello Neural Net with following configuration\n",
    "  nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 1\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 2\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 3\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 4\n",
    "  nn.BatchNorm2d(args.num_channels) X 4\n",
    "  nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024) # Fully-connected Layer 1\n",
    "  nn.Linear(1024, 512) # Fully-connected Layer 2\n",
    "  nn.Linear(512, self.action_size) # Fully-connected Layer 3\n",
    "  nn.Linear(512, 1) # Fully-connected Layer 4\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, args):\n",
    "    \"\"\"\n",
    "    Initialise game parameters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.args = args\n",
    "\n",
    "    super(OthelloNNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1,\n",
    "                           padding=1)\n",
    "    self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "    self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn3 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn4 = nn.BatchNorm2d(args.num_channels)\n",
    "\n",
    "    self.fc1 = nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024)\n",
    "    self.fc_bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    self.fc2 = nn.Linear(1024, 512)\n",
    "    self.fc_bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "    self.fc3 = nn.Linear(512, self.action_size)\n",
    "\n",
    "    self.fc4 = nn.Linear(512, 1)\n",
    "\n",
    "  def forward(self, s):\n",
    "    \"\"\"\n",
    "    Controls forward pass of OthelloNNet\n",
    "\n",
    "    Args:\n",
    "      s: np.ndarray\n",
    "        Array of size (batch_size x board_x x board_y)\n",
    "\n",
    "    Returns:\n",
    "      Probability distribution over actions at the current state and the value of the current state.\n",
    "    \"\"\"\n",
    "    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4)) # reshaping of\n",
    "\n",
    "    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
    "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "    pi = self.fc3(s)  # batch_size x action_size\n",
    "    v = self.fc4(s)   # batch_size x 1\n",
    "\n",
    "    # Returns probability distribution over actions at the current state and the value of the current state.\n",
    "    return F.log_softmax(pi, dim=1), torch.tanh(v)\n",
    "\n",
    "class ValueBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Value Based Player\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, vnet):\n",
    "    \"\"\"\n",
    "    Initialise value based player parameters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      vnet: Value Network instance\n",
    "        Instance of the Value Network class above;\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.vnet = vnet\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate game play\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      candidates: List\n",
    "        Collection of tuples describing action and values of future predicted states\n",
    "    \"\"\"\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    candidates = []\n",
    "    max_num_actions = 4\n",
    "    va = np.where(valids)[0]\n",
    "    va_list = va.tolist()\n",
    "    random.shuffle(va_list)\n",
    "    for a in va_list:\n",
    "      # Return next board state using getNextState() function\n",
    "      nextBoard, _ = self.game.getNextState(board, 1, a)\n",
    "      # Predict the value of next state using value network\n",
    "      value = self.vnet.predict(nextBoard)\n",
    "      # Add the value and the action as a tuple to the candidate lists, note that you might need to change the sign of the value based on the player\n",
    "      candidates += [(-value, a)]\n",
    "\n",
    "      if len(candidates) == max_num_actions:\n",
    "        break\n",
    "\n",
    "    # Sort by the values\n",
    "    candidates.sort()\n",
    "\n",
    "    # Return action associated with highest value\n",
    "    return candidates[0][1]\n",
    "\n",
    "class ValueNetwork(NeuralNet):\n",
    "  \"\"\"\n",
    "  Initiates the Value Network\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game):\n",
    "    \"\"\"\n",
    "    Initialise network parameters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.nnet.to(args.device)\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    Function to train value network\n",
    "\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples with each example is of form (board, pi, v)\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        v_losses = []   # To store the losses per epoch\n",
    "        batch_count = int(len(examples) / args.batch_size)  # len(examples)=200, batch-size=64, batch_count=3\n",
    "        t = tqdm(range(batch_count), desc='Training Value Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # Read the ground truth information from MCTS simulation using the loaded examples\n",
    "          boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # Length of boards, pis, vis = 64\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "          # Predict\n",
    "          # To run on GPU if available\n",
    "          boards, target_vs = boards.contiguous().to(args.device), target_vs.contiguous().to(args.device)\n",
    "\n",
    "          # Compute output\n",
    "          _, out_v = self.nnet(boards)\n",
    "          l_v = self.loss_v(target_vs, out_v)  # Total loss\n",
    "\n",
    "          # Record loss\n",
    "          v_losses.append(l_v.item())\n",
    "          t.set_postfix(Loss_v=l_v.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          l_v.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    Function to perform prediction\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      v: OthelloNet instance\n",
    "        Data of the OthelloNet class instance above;\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "        _, v = self.nnet(board)\n",
    "    return v.data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_v(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Mean squared error\n",
    "\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth variables corresponding to input\n",
    "      outputs: np.ndarray\n",
    "        Predictions of Network\n",
    "\n",
    "    Returns:\n",
    "      MSE Loss calculated as: square of the difference between your model's predictions\n",
    "      and the ground truth and average across the whole dataset\n",
    "    \"\"\"\n",
    "    # Mean squared error (MSE)\n",
    "    return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Code Checkpointing\n",
    "\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(folder):\n",
    "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "      os.mkdir(folder)\n",
    "    else:\n",
    "      print(\"Checkpoint Directory exists! \")\n",
    "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
    "    print(\"Model saved! \")\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Load code checkpoint\n",
    "\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "      raise (\"No model in path {}\".format(filepath))\n",
    "\n",
    "    checkpoint = torch.load(filepath, map_location=args.device)\n",
    "    self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "def loadTrainExamples(folder, filename):\n",
    "  \"\"\"\n",
    "  Helper function to load Training examples\n",
    "\n",
    "  Args:\n",
    "    folder: string\n",
    "      Path specifying training examples\n",
    "    filename: string\n",
    "      File name of training examples\n",
    "\n",
    "  Returns:\n",
    "    trainExamplesHistory: list\n",
    "      Returns examples based on the model were already collected (loaded)\n",
    "  \"\"\"\n",
    "  trainExamplesHistory = []\n",
    "  modelFile = os.path.join(folder, filename)\n",
    "  examplesFile = modelFile + \".examples\"\n",
    "  if not os.path.isfile(examplesFile):\n",
    "    print(f'File \"{examplesFile}\" with trainExamples not found!')\n",
    "    r = input(\"Continue? [y|n]\")\n",
    "    if r != \"y\":\n",
    "      sys.exit()\n",
    "  else:\n",
    "    print(\"File with train examples found. Loading it...\")\n",
    "    with open(examplesFile, \"rb\") as f:\n",
    "      trainExamplesHistory = Unpickler(f).load()\n",
    "    print('Loading done!')\n",
    "    return trainExamplesHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The hyperparameters used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'numIters': 1,            # In training, number of iterations = 1000 and num of episodes = 100\n",
    "    'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,      # To control exploration and exploitation\n",
    "    'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n",
    "    'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n",
    "    'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "    'maxDepth':5,             # Maximum number of rollouts\n",
    "    'numMCsims': 5,           # Number of monte carlo simulations\n",
    "    'mc_topk': 3,             # Top k actions for monte carlo rollout\n",
    "\n",
    "    'checkpoint': './temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "    # Define neural network arguments\n",
    "    'lr': 0.001,               # lr: Learning Rate\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'device': DEVICE,\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load in trained value network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Load in trained value network\n",
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "# Alternative if the downloading of trained model didn't work (will train the model)\n",
    "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "\n",
    "    path = \"nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "    loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')\n",
    "\n",
    "    set_seed(seed=SEED)\n",
    "    game = OthelloGame(6)\n",
    "    vnet = ValueNetwork(game)\n",
    "    vnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "A reminder of the network architecture\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGames/static/CNN.jpg\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Train a policy network from expert game data\n",
    "\n",
    "*Time estimate: ~25mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Goal**: How to train a policy network via supervised learning / behavioural cloning.\n",
    "\n",
    "**Exercise**:\n",
    "* Train a network to predict the next move in an expert dataset by maximizing the log likelihood of the next action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 1: Train a policy network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Train a policy network\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1hQ4y127GJ\", width=730, height=410, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"vj9gKNJ19D8\", width=730, height=410, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 1: Train a policy network')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 1: Implement `PolicyNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In section 3 we simply chose to move based on the highest predicted value of the next step. Here, we will use a different approach. We will train a network to directly produce a policy function as a distribution over all possible discrete actions, given the current state. Learning will be based  on expert moves; thus, we call this behavioral cloning. \n",
    "\n",
    "We will use the exact same network that we have used above for the value function learning. But now we will train the network explicitly on every single move of expert players. \n",
    "\n",
    "For computing our objective function, we will use the negative log-likelihood of targets $t_i$ by using the cross-entropy function:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{CE} = - \\frac{1}{N} \\sum_i^N t_i \\cdot \\log(output_i)\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: remember that the OthelloNet already returns the **Log**-softmax of the output from the 3rd linear layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(NeuralNet):\n",
    "  \"\"\"\n",
    "  Initialise Policy Network\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game):\n",
    "    \"\"\"\n",
    "    Initalise policy network paramaters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.nnet.to(args.device)\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    Function for Policy Network Training\n",
    "\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples where each example is of form (board, pi, v)\n",
    "\n",
    "    Return:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        pi_losses = []\n",
    "\n",
    "        batch_count = int(len(examples) / args.batch_size)\n",
    "\n",
    "        t = tqdm(range(batch_count), desc='Training Policy Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "          boards, pis, _ = list(zip(*[examples[i] for i in sample_ids]))\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_pis = torch.FloatTensor(np.array(pis))\n",
    "\n",
    "          # Predict\n",
    "          boards, target_pis = boards.contiguous().to(args.device), target_pis.contiguous().to(args.device)\n",
    "\n",
    "          #################################################\n",
    "          ## TODO for students: ##\n",
    "          ## 1. Compute the policy (pi) predicted by OthelloNNet() ##\n",
    "          ## 2. Implement the loss_pi() function below and then use it to update the policy loss. ##\n",
    "          # Fill out function and remove\n",
    "          raise NotImplementedError(\"Compute the output\")\n",
    "          #################################################\n",
    "          # Compute output\n",
    "          out_pi, _ = ...\n",
    "          l_pi = ...\n",
    "\n",
    "          # Record loss\n",
    "          pi_losses.append(l_pi.item())\n",
    "          t.set_postfix(Loss_pi=l_pi.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          l_pi.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    Function to perform prediction\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      Data from the OthelloNet class instance above;\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "      pi,_ = self.nnet(board)\n",
    "    return torch.exp(pi).data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_pi(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Negative Log Likelihood(NLL) of Targets\n",
    "\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth variables corresponding to input\n",
    "      outputs: np.ndarray\n",
    "        Predictions of Network\n",
    "\n",
    "    Returns:\n",
    "      Negative Log Likelihood calculated as: When training a model, we aspire to find the minima of a\n",
    "      loss function given a set of parameters (in a neural network, these are the weights and biases).\n",
    "      Sum the loss function to all the correct classes. So, whenever the network assigns high confidence at\n",
    "      the correct class, the NLL is low, but when the network assigns low confidence at the correct class,\n",
    "      the NLL is high.\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## TODO for students: To implement the loss function, please compute and return the negative log likelihood of targets.\n",
    "    ## For more information, here is a reference that connects the expression to the neg-log-prob: https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Compute the loss\")\n",
    "    #################################################\n",
    "    return ...\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Code Checkpointing\n",
    "\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(folder):\n",
    "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "      os.mkdir(folder)\n",
    "    else:\n",
    "      print(\"Checkpoint Directory exists! \")\n",
    "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
    "    print(\"Model saved! \")\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Load code checkpoint\n",
    "\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "      raise (\"No model in path {}\".format(filepath))\n",
    "    checkpoint = torch.load(filepath, map_location=args.device)\n",
    "    self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 1: Implement PolicyNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D5_ReinforcementLearningForGames/solutions/W3D5_Tutorial3_Solution_7c7e4c5e.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Train the policy network\n",
    "\n",
    "**Important:** Only run this cell if you do not have access to the pretrained models in the `rl_for_games` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  pnet = PolicyNetwork(game)\n",
    "  pnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Use a trained policy network to play games\n",
    "\n",
    "Time estimate: ~25mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Goal**: Use a policy network to play games.\n",
    "\n",
    "**Exercise:** \n",
    "* Use the policy network to give probabilities for the next move.\n",
    "* Build a player that takes the move given the maximum probability by the network.\n",
    "* Compare this to another player that samples moves according to the probability distribution output by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 2: Play games using a policy network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Play games using a policy network\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1aq4y1S7o4\", width=730, height=410, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"yHtVqT2Nstk\", width=730, height=410, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 2: Play games using a policy network')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note**: in the video's softmax function, $T=1$ is the softmax kernel and $z_i$ is the networks output before softmax transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 2: Implement the `PolicyBasedPlayer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First we initialize the game and load in the pre-trained policy net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next we create our policy-based player by using the policy network to produce a set of action probabilities for all valid board positions. \n",
    "\n",
    "There are at least 2 ways then to choose the next action:\n",
    "1. sampling-based player: we sample from the action probability distribution. This will result in actions with higher probabilities to be randomly selected more often than actions with lower probabilities.\n",
    "2. \"greedy\" player: we always choose the action with the highest action probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class PolicyBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Policy Based Player\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, pnet, greedy=True):\n",
    "    \"\"\"\n",
    "    Initialize Policy based player parameters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      pnet: Policy Network instance\n",
    "        Instance of the Policy Network class above\n",
    "      greedy: Boolean\n",
    "        If true, implement greedy approach\n",
    "        Else, implement random sample policy based player\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.pnet = pnet\n",
    "    self.greedy = greedy\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate game play\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      a: np.ndarray\n",
    "        If greedy, implement greedy policy player\n",
    "        Else, implement random sample policy based player\n",
    "    \"\"\"\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    #################################################\n",
    "    ## TODO for students:  ##\n",
    "    ## 1. Compute the action probabilities using policy network pnet()\n",
    "    ## 2. Mask invalid moves (set their action probability to 0) using valids variable and the action probabilites computed above.\n",
    "    ## 3. Compute the sum over the probabilities of the valid actions and store them in sum_vap.\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Define the play\")\n",
    "    #################################################\n",
    "    action_probs = ...\n",
    "    vap = ...  # Masking invalid moves\n",
    "    sum_vap = ...\n",
    "\n",
    "    if sum_vap > 0:\n",
    "      vap /= sum_vap  # Renormalize\n",
    "    else:\n",
    "      # If all valid moves were masked we make all valid moves equally probable\n",
    "      print(\"All valid moves were masked, doing a workaround.\")\n",
    "      vap = vap + valids\n",
    "      vap /= np.sum(vap)\n",
    "\n",
    "    if self.greedy:\n",
    "      # Greedy policy player\n",
    "      a = np.where(vap == np.max(vap))[0][0]\n",
    "    else:\n",
    "      # Sample-based policy player\n",
    "      a = np.random.choice(self.game.getActionSize(), p=vap)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 5: Implement the PolicyBasedPlayer')\n",
    "\n",
    "# Playing games\n",
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "player1 = PolicyBasedPlayer(game, pnet, greedy=True).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "## Uncomment below to test!\n",
    "# result = arena.playGames(num_games, verbose=False)\n",
    "# print(f\"\\n\\n{result}\")\n",
    "# win_rate_player1 = result[0] / num_games\n",
    "# print(f\"\\nWin rate for greedy policy player 1 (vs random player 2) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D5_ReinforcementLearningForGames/solutions/W3D5_Tutorial3_Solution_738279ec.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    " Win rate for greedy policy player 1 (vs random player 2) over 20 games: 80.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 3: Player comparisons\n",
    "\n",
    "Time estimate: ~10mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next we want to compare how our different players fair, i.e. random vs. value-based vs. policy-based (greedy or sampling-based)... Feel free to explore some of the comparisons we have not explicitly provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Comparing a sampling-based policy based player versus a random player\n",
    "\n",
    "There's often randomness in the results as we are running the players for a low number of games (only 20 games due to compute + time costs). So, when students are running the cells they might not get the expected result. To better measure the strength of players you can run more games!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet, greedy=False).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"Win rate for sample-based policy based player 1 (vs random player 2) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Win rate for sample-based policy based player 1 (vs random player 2) over 20 games: 95.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Compare greedy policy based player versus value based player "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet).play\n",
    "player2 = ValueBasedPlayer(game, vnet).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"Win rate for greedy policy based player 1 vs value based player) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Compare greedy policy based player versus sampling-based policy player "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet).play # greedy player\n",
    "player2 = PolicyBasedPlayer(game, pnet, greedy=False).play # sample-based player\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"Win rate for greedy policy player 1 (vs sample based policy player) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We've been diving into the code so take a few minutes to recap what the different players are with your group and how they're choosing their actions (random player, value player, greedy policy player, sample-based policy player)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: Ethical aspects\n",
    "\n",
    "*Time estimate: ~5mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 3: Unstoppable opponents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Unstoppable opponents\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1WA411w7mw\", width=730, height=410, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"q7181lvoNpM\", width=730, height=410, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 3: Unstoppable opponents')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, you have learned about policy-based players and compared them to random and value-based players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Airtable Submission Link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Airtable Submission Link\n",
    "from IPython import display as IPydisplay\n",
    "IPydisplay.HTML(\n",
    "   f\"\"\"\n",
    " <div>\n",
    "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
    "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/AirtableSubmissionButton.png?raw=1\"\n",
    " alt=\"button link to Airtable\" style=\"width:410px\"></a>\n",
    "   </div>\"\"\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W3D5_Tutorial3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}